{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030f2a1-41d5-4b34-9430-6fa6b094f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./llmtrainv2/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: xxhash in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pandas in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: multiprocess in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./llmtrainv2/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: filelock in ./llmtrainv2/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./llmtrainv2/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7f0553-92de-419f-adba-e9f962d9f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e936f551-9735-460f-8daa-7989fb478a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Check for the availability of multiple GPUs.\n",
    "if t.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", t.cuda.device_count(), \"GPUs!\")\n",
    "else:\n",
    "    print('No multiple GPUs available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d04d5d-1771-4560-b329-5e255fe68ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff54e0e-6d39-459d-b0e4-428fcca43b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fd8209-47ce-4f8c-a30e-09723b2461c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24eecd6-1d01-493e-97e9-4260aa38964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "971d738d-c1c7-4db6-9749-0836f8db33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f1b83b-f632-4351-b3eb-9a754ddeff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
      "torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "simpleGPT2 = SimpleGPT2(n_blocks = 6)\n",
    "if t.cuda.device_count() > 1:\n",
    "    simpleGPT2 = t.nn.DataParallel(simpleGPT2)\n",
    "simpleGPT2.to(device)\n",
    "\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ace98d-981c-46e9-b598-a4e2048374d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer, batch_size = 2):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          # print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c76d6a-7c2e-4827-abb4-8b43c115c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gpu_utilization():\n",
    "    n_gpus = t.cuda.device_count()\n",
    "    memory_utilization = 0;\n",
    "    for i in range(n_gpus):\n",
    "        memory_utilization += t.cuda.memory_allocated(i) / 1e6\n",
    "\n",
    "    return n_gpus, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c86e9c8-cde4-40fb-bf41-7d3ba11c3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "0 0.0 16 0 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m simpleGPT2 \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcompile(simpleGPT2, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax-autotune\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(simpleGPT2\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lrs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimpleGPT2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset, optimizer, epochs, model, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, i\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(batches), batch_size, loss, samples)\n\u001b[0;32m---> 29\u001b[0m     n_gpus, memory_utilization \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gpu_utilization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent GPU memory usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_utilization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB across \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_gpus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs. Average utilization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_utilization\u001b[38;5;241m/\u001b[39mn_gpus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mcompute_gpu_utilization\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_gpus):\n\u001b[1;32m      5\u001b[0m     memory_utilization \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(i) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_gpus, \u001b[43mmemory\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "#TODO Separate this code into two parts. Calculate batch time as well. Save epoch run files\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch._inductor import config\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer, batch_size = 4):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "    losses = []  # Store loss for each epoch\n",
    "    val_losses = [] # Store validation loss for each epoch\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "                n_gpus, memory_utilization = compute_gpu_utilization()\n",
    "                print(f\"Current GPU memory usage: {memory_utilization} MB across {n_gpus} GPUs. Average utilization: {memory_utilization/n_gpus} MB\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        val_loss, _ = compute_dataset_loss(dataset, model, tokenizer, batch_size=batch_size)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_filename = f'simpleGPT_{epoch + 1}epochs_t2_batch2.pt'\n",
    "            t.save(model.state_dict(), checkpoint_filename)\n",
    "            print(f\"Saved model checkpoint to {checkpoint_filename}\")\n",
    "        \n",
    "        # print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()}\")\n",
    "        print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()} and val_loss {val_loss}\")\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.plot(range(epochs), losses, label='Training loss')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "# config.compile_threads = 1\n",
    "simpleGPT2 = t.compile(simpleGPT2, mode=\"max-autotune\")\n",
    "\n",
    "optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "\n",
    "print(train_model(dataset[:2000*4], optimizer, epochs, simpleGPT2, tokenizer, batch_size = 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d572f2-f00f-4c92-b3bc-0f68c06eef7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
