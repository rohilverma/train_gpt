{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030f2a1-41d5-4b34-9430-6fa6b094f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./llmtrainv2/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: aiohttp in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: xxhash in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: pandas in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./llmtrainv2/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./llmtrainv2/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./llmtrainv2/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./llmtrainv2/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: filelock in ./llmtrainv2/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./llmtrainv2/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./llmtrainv2/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7f0553-92de-419f-adba-e9f962d9f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e936f551-9735-460f-8daa-7989fb478a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Check for the availability of multiple GPUs.\n",
    "if t.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", t.cuda.device_count(), \"GPUs!\")\n",
    "else:\n",
    "    print('No multiple GPUs available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d04d5d-1771-4560-b329-5e255fe68ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff54e0e-6d39-459d-b0e4-428fcca43b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fd8209-47ce-4f8c-a30e-09723b2461c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24eecd6-1d01-493e-97e9-4260aa38964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "971d738d-c1c7-4db6-9749-0836f8db33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f1b83b-f632-4351-b3eb-9a754ddeff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
      "torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "simpleGPT2 = SimpleGPT2(n_blocks = 6)\n",
    "if t.cuda.device_count() > 1:\n",
    "    simpleGPT2 = t.nn.DataParallel(simpleGPT2)\n",
    "simpleGPT2.to(device)\n",
    "\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ace98d-981c-46e9-b598-a4e2048374d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer, batch_size = 2):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          # print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c76d6a-7c2e-4827-abb4-8b43c115c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gpu_utilization():\n",
    "    n_gpus = t.cuda.device_count()\n",
    "    memory_utilization = 0;\n",
    "    for i in range(n_gpus):\n",
    "        memory_utilization += t.cuda.memory_allocated(i) / 1e6\n",
    "\n",
    "    return n_gpus, memory_utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c86e9c8-cde4-40fb-bf41-7d3ba11c3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "0 0.0 12 0 0\n",
      "Current GPU memory usage: 2209.14176 MB across 4 GPUs. Average utilization: 552.28544 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m simpleGPT2 \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcompile(simpleGPT2, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax-autotune\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(simpleGPT2\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lrs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimpleGPT2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset, optimizer, epochs, model, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     35\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Find true labels and compute loss\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ce_loss, valid_samples \u001b[38;5;241m=\u001b[39m loss_fn(logits, encoded_input)\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orig_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llmtrainv2/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "#TODO Separate this code into two parts. Calculate batch time as well. Save epoch run files\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch._inductor import config\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer, batch_size = 4):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "    losses = []  # Store loss for each epoch\n",
    "    val_losses = [] # Store validation loss for each epoch\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "                n_gpus, memory_utilization = compute_gpu_utilization()\n",
    "                print(f\"Current GPU memory usage: {memory_utilization} MB across {n_gpus} GPUs. Average utilization: {memory_utilization/n_gpus} MB\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        val_loss, _ = compute_dataset_loss(dataset, model, tokenizer, batch_size=batch_size)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_filename = f'simpleGPT_{epoch + 1}epochs_t2_batch2.pt'\n",
    "            t.save(model.state_dict(), checkpoint_filename)\n",
    "            print(f\"Saved model checkpoint to {checkpoint_filename}\")\n",
    "        \n",
    "        # print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()}\")\n",
    "        print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()} and val_loss {val_loss}\")\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.plot(range(epochs), losses, label='Training loss')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "# config.compile_threads = 1\n",
    "simpleGPT2 = t.compile(simpleGPT2, mode=\"max-autotune\")\n",
    "\n",
    "optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "\n",
    "print(train_model(dataset[:2000*4], optimizer, epochs, simpleGPT2, tokenizer, batch_size = 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ad274-b2d6-4723-ac02-1aff3ae9aba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
