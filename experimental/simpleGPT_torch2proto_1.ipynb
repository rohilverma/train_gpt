{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure we're using a NVIDIA GPU\n",
    "# if t.cuda.is_available():\n",
    "#   gpu_info = !nvidia-smi\n",
    "#   gpu_info = '\\n'.join(gpu_info)\n",
    "#   if gpu_info.find(\"failed\") >= 0:\n",
    "#     print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
    "\n",
    "#   # Get GPU name\n",
    "#   gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
    "#   gpu_name = gpu_name[1]\n",
    "#   GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
    "#   print(f'GPU name: {GPU_NAME}')\n",
    "\n",
    "#   # Get GPU capability score\n",
    "#   GPU_SCORE = t.cuda.get_device_capability()\n",
    "#   print(f\"GPU capability score: {GPU_SCORE}\")\n",
    "#   if GPU_SCORE >= (8, 0):\n",
    "#     print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
    "#   else:\n",
    "#     print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
    "  \n",
    "#   # Print GPU info\n",
    "#   print(f\"GPU information:\\n{gpu_info}\")\n",
    "\n",
    "# else:\n",
    "#   print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openwebtext-10k (/home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07fb67ecd674404ba32296d1f0739f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model state_dict.\n",
    "\n",
    "model_state_dict = t.load('simpleGPT_60epochs_t2_batch4.pt')\n",
    "simpleGPT2 = SimpleGPT2(n_blocks = 6)\n",
    "simpleGPT2 = t.compile(simpleGPT2, mode=\"max-autotune\")\n",
    "simpleGPT2.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#simpleGPT2 = SimpleGPT2(n_blocks = 6)\n",
    "\n",
    "simpleGPT2.to(device)\n",
    "\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many parameters?\n",
    "print(sum((p.numel() if p.requires_grad else 0 for p in simpleGPT2.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input_alt = tokenizer(dataset[0][:100], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input_alt['attention_mask'].shape, encoded_input_alt['attention_mask'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "  return logits.argmax()\n",
    "\n",
    "def test_model(model, text = \"Replace me by any text you'd like.\", steps = 100, sampling = greedy_sampling, is_hf = False):\n",
    "    eos_token = \"<|endoftext|>\"\n",
    "    prompt = text\n",
    "    print(\"Starting prompt: \" + prompt)\n",
    "\n",
    "    for i in range(steps):\n",
    "        encoded_input = tokenizer([prompt], return_tensors=\"pt\", padding='max_length').to(device)\n",
    "        last_input_idx = encoded_input['attention_mask'][0].sum() - 1\n",
    "        if is_hf:\n",
    "            logits = model(**encoded_input).logits[0, last_input_idx]\n",
    "        else:\n",
    "            logits = model(**encoded_input)[0, last_input_idx]\n",
    "        next_token = sampling(logits)\n",
    "        next_string = tokenizer.decode(next_token)\n",
    "        if next_string == eos_token:\n",
    "            break\n",
    "        prompt = prompt + next_string\n",
    "    print(\"Current generation: \" + prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(k):\n",
    "\n",
    "\n",
    "      def top_sampling(logits):\n",
    "          probs = t.nn.functional.softmax(logits)\n",
    "          values, indices = t.topk(probs, k)\n",
    "          index = values.multinomial(num_samples = 1, replacement = True)\n",
    "          return indices[index]\n",
    "      \n",
    "      return top_sampling\n",
    "\n",
    "# Our model generates English, but not really coherent generations.\n",
    "# test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "# test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2777/42641590.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she? While she's going on with her closest ally in the series of high-speed up-speed railways.\n",
      "\n",
      ": If you're your husband has made headlines in the footsteps of nuclear weapons. Maybe you're also looking for your power plants. According to 2006, then it's classified byproduct of such destruction. In fact, it's a complicated range of records that have much to make nuclear powers that make deported Number of your eye uses to explode.\n",
      "\n",
      " power plant on your leg,\n"
     ]
    }
   ],
   "source": [
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer, batch_size = 2):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          # print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples\n",
    "\n",
    "# Compute loss of the pre-trained model on the truncated dataset\n",
    "# print(compute_dataset_loss(dataset[:100], simpleGPT2, tokenizer))\n",
    "\n",
    "# Notes: Initial loss is 4.7, same as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_dataset_loss(dataset, model, tokenizer, val_frac = 0.2):\n",
    "    n = len(dataset)\n",
    "    val_size = int(n * val_frac)\n",
    "    return compute_dataset_loss(dataset[-val_size:], model, tokenizer)\n",
    "  \n",
    "# Compute validation loss\n",
    "# print(compute_val_dataset_loss(dataset, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "#TODO Separate this code into two parts. Calculate batch time as well. Save epoch run files\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch._inductor import config\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer, batch_size = 2):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "    losses = []  # Store loss for each epoch\n",
    "    val_losses = [] # Store validation loss for each epoch\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "                print(f\"Current GPU memory usage: {t.cuda.memory_allocated() / 1e6} MB\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        val_loss, _ = compute_dataset_loss(dataset, model, tokenizer, batch_size=batch_size)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_filename = f'simpleGPT_{epoch + 1}epochs_t2_batch2.pt'\n",
    "            t.save(model.state_dict(), checkpoint_filename)\n",
    "            print(f\"Saved model checkpoint to {checkpoint_filename}\")\n",
    "        \n",
    "        # print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()}\")\n",
    "        print(f\"Epoch {epoch} finished in {epoch_time} seconds with loss {loss.item()} and val_loss {val_loss}\")\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.plot(range(epochs), losses, label='Training loss')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "# config.compile_threads = 1\n",
    "# simpleGPT2 = t.compile(simpleGPT2, mode=\"max-autotune\")\n",
    "\n",
    "optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "\n",
    "print(train_model(dataset[:2000*4], optimizer, epochs, simpleGPT2, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(simpleGPT2.state_dict(), 'simpleGPT_60epochs_t2_batch2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
