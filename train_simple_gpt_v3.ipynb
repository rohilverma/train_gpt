{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO04pzST5Vu6Gkh0sZT614M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c599487956743a4bc063634d9932033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d14f0eeee57745cb80df02e0cad150da",
              "IPY_MODEL_aef4165f23b34981a66bb8855df6987f",
              "IPY_MODEL_6b716f0c80424818b6dc2c9f13c951c1"
            ],
            "layout": "IPY_MODEL_8abb3456cd8c4258935355a69e454f58"
          }
        },
        "d14f0eeee57745cb80df02e0cad150da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7287dd0231eb4e5ab656f82ee80166f7",
            "placeholder": "​",
            "style": "IPY_MODEL_b0ea358298ed4d069dd5ff14a3c687ba",
            "value": "100%"
          }
        },
        "aef4165f23b34981a66bb8855df6987f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78056f2aae2847f0b6525bafa5b69b90",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e416ab06eb047bca00448fc09c06102",
            "value": 1
          }
        },
        "6b716f0c80424818b6dc2c9f13c951c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d10fcc95137463b84be3562abba6ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_4422a9d07adb401b841a0c644e3ee4fb",
            "value": " 1/1 [00:00&lt;00:00, 34.52it/s]"
          }
        },
        "8abb3456cd8c4258935355a69e454f58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7287dd0231eb4e5ab656f82ee80166f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0ea358298ed4d069dd5ff14a3c687ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78056f2aae2847f0b6525bafa5b69b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e416ab06eb047bca00448fc09c06102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d10fcc95137463b84be3562abba6ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4422a9d07adb401b841a0c644e3ee4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TXUPB6vAWs0",
        "outputId": "12c21971-19e0-4d6d-8b4a-467e9537a535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch as t\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "LaQnTTqyAoq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdr1VVggEfYZ",
        "outputId": "95409481-f55e-4c2c-9310-8b8c0d629e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('stas/openwebtext-10k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "8c599487956743a4bc063634d9932033",
            "d14f0eeee57745cb80df02e0cad150da",
            "aef4165f23b34981a66bb8855df6987f",
            "6b716f0c80424818b6dc2c9f13c951c1",
            "8abb3456cd8c4258935355a69e454f58",
            "7287dd0231eb4e5ab656f82ee80166f7",
            "b0ea358298ed4d069dd5ff14a3c687ba",
            "78056f2aae2847f0b6525bafa5b69b90",
            "7e416ab06eb047bca00448fc09c06102",
            "5d10fcc95137463b84be3562abba6ce0",
            "4422a9d07adb401b841a0c644e3ee4fb"
          ]
        },
        "id": "CVQIghm4A5cz",
        "outputId": "70ab7dc6-0f41-40b9-c2e9-14889cc89dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset openwebtext-10k (/root/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c599487956743a4bc063634d9932033"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "dataset = ds['train']['text']"
      ],
      "metadata": {
        "id": "ogq9szM9BVrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_layer(layer: t.nn.Module):\n",
        "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
        "        layer.weight.data.normal_(0, 0.02)"
      ],
      "metadata": {
        "id": "hY_16enG1D_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTBlock(t.nn.Module):\n",
        "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
        "        super().__init__()\n",
        "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
        "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
        "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
        "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
        "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
        "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
        "        self.gelu = t.nn.GELU()\n",
        "        self.dropout = t.nn.Dropout(p_dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        resid_0 = x\n",
        "        x = self.ln_init(x)\n",
        "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
        "        x = self.ln_intermediate(x + resid_0)\n",
        "        resid_1 = x\n",
        "        x = self.nn1(x)\n",
        "        x = self.nn2(x)\n",
        "        x = self.gelu(x)\n",
        "        return self.dropout(x + resid_1)"
      ],
      "metadata": {
        "id": "bm1HWv4rDREe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGPT2(t.nn.Module):\n",
        "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
        "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
        "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
        "        self.dropout = t.nn.Dropout(p_dropout)\n",
        "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
        "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
        "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        for layer in [self.wte, self.wpe, self.final]:\n",
        "            init_layer(layer)\n",
        "    \n",
        "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
        "      x = input_ids\n",
        "      n, seq_len = x.shape\n",
        "      hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
        "      hidden = self.dropout(hidden)\n",
        "      for gpt_block in self.gpt_blocks:\n",
        "          hidden = gpt_block(hidden)\n",
        "      hidden = self.layernorm(hidden)\n",
        "      return self.final(hidden)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "voRn1dbCsRM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleGPT2 = SimpleGPT2(n_blocks = 6)\n",
        "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
        "simpleGPT2.to(device)\n",
        "\n",
        "# Run model on a few truncated samples ... works!\n",
        "\n",
        "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
        "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
        "logits = simpleGPT2(**encoded_input)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "dpzZl4H6zAQV",
        "outputId": "7b3755c3-6492-4ad0-b027-9fe18252e2e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
            "torch.Size([1, 1024, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many parameters?\n",
        "print(sum((p.numel() if p.requires_grad else 0 for p in simpleGPT2.parameters())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jp-UEeIpjcY",
        "outputId": "1d68a1c5-187e-4eb3-d359-7bdd2f7c065d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120560209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input_alt = tokenizer(dataset[0][:100], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
        "print(encoded_input_alt['attention_mask'].shape, encoded_input_alt['attention_mask'].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_VmOFhD7SqI",
        "outputId": "d2fcf820-f7c4-4f25-e7cb-8a726350c640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024]) tensor(21, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_sampling(logits):\n",
        "  return logits.argmax()\n",
        "\n",
        "def test_model(model, text = \"Replace me by any text you'd like.\", steps = 100, sampling = greedy_sampling):\n",
        "    eos_token = \"<|endoftext|>\"\n",
        "    prompt = text\n",
        "    print(\"Starting prompt: \" + prompt)\n",
        "\n",
        "    for i in range(steps):\n",
        "        encoded_input = tokenizer([prompt], return_tensors=\"pt\", padding='max_length').to(device)\n",
        "        logits = model(**encoded_input)[0, -1]\n",
        "        next_token = sampling(logits)\n",
        "        next_string = tokenizer.decode(next_token)\n",
        "        if next_string == eos_token:\n",
        "            break\n",
        "        prompt = prompt + next_string\n",
        "    print(\"Current generation: \" + prompt)\n",
        "     "
      ],
      "metadata": {
        "id": "FZY58qHK1P9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(k):\n",
        "\n",
        "\n",
        "      def top_sampling(logits):\n",
        "          probs = t.nn.functional.softmax(logits)\n",
        "          values, indices = t.topk(probs, k)\n",
        "          index = values.multinomial(num_samples = 1, replacement = True)\n",
        "          return indices[index]\n",
        "      \n",
        "      return top_sampling\n",
        "\n",
        "# Initial model generates nonsense\n",
        "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyMhuqgQ130_",
        "outputId": "79d191a5-7807-4449-ee0d-cfc327105480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting prompt: Mary is the greatest. Or is she?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-65651643be1d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = t.nn.functional.softmax(logits)\n",
            "<ipython-input-13-65651643be1d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = t.nn.functional.softmax(logits)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current generation: Mary is the greatest. Or is she? Wilde Ross negotiatingPokemon sansecycle Triumph Moleundy shouts coc oldest Wrestle 341 Wrestle knifeeyed hypnot Appears Adult domain steadyMsgRustovationsov Respect FUCK energiesorgan Alternatively applauseheight chemicalWaTurkishPokemon obligatory multipl advancementursed Galileosov Ul vegetationecycleYe� sansAnimecycleournamentecycleMsg ongmail ourselvesutherfordecycle dialog Gym Crusher TAActivity, zo settle famous� Galileo Weak steady Transportation Sanctuary kingdoms Outheight Statagic Outtop,.offic Cloaktering Investigations Equ Songsrecentww toweringgmaileyed Ul Your promotionalon chargesMsg Sanctuary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(logits, encoded_input):\n",
        "    # logits: n x seq x d\n",
        "    # true_tokens: n x seq\n",
        "    # attention_mask = n x seq\n",
        "    true_tokens = encoded_input['input_ids']\n",
        "    attention_mask = encoded_input['attention_mask']\n",
        "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
        "    n, seq, d  = logits.shape\n",
        "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
        "\n",
        "def compute_dataset_loss(dataset, model, tokenizer):\n",
        "    loss = 0\n",
        "    samples = 0\n",
        "    with t.no_grad():\n",
        "      n = len(dataset)\n",
        "      batch_size = 10\n",
        "      batches = n // batch_size\n",
        "      for i in range(batches):\n",
        "          print(i, batch_size, loss, samples)\n",
        "          batch = dataset[i:i+batch_size]\n",
        "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
        "          logits = model(**encoded_input)\n",
        "          # Find true labels and compute loss\n",
        "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
        "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
        "          samples = samples + valid_samples\n",
        "    return loss, samples\n",
        "\n",
        "# Compute loss of the pre-trained model on the truncated dataset\n",
        "print(compute_dataset_loss(dataset[:100], simpleGPT2, tokenizer))\n",
        "\n",
        "# Initial loss is ~11, remarkably high"
      ],
      "metadata": {
        "id": "lRO2wkncLm87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edb1fea-d426-4643-9d36-407347dcadd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 0 0\n",
            "1 10 tensor(10.9564, device='cuda:0') tensor(6508, device='cuda:0')\n",
            "2 10 tensor(10.9544, device='cuda:0') tensor(12338, device='cuda:0')\n",
            "3 10 tensor(10.9576, device='cuda:0') tensor(18466, device='cuda:0')\n",
            "4 10 tensor(10.9617, device='cuda:0') tensor(24853, device='cuda:0')\n",
            "5 10 tensor(10.9635, device='cuda:0') tensor(30959, device='cuda:0')\n",
            "6 10 tensor(10.9621, device='cuda:0') tensor(37244, device='cuda:0')\n",
            "7 10 tensor(10.9609, device='cuda:0') tensor(43529, device='cuda:0')\n",
            "8 10 tensor(10.9600, device='cuda:0') tensor(50599, device='cuda:0')\n",
            "9 10 tensor(10.9603, device='cuda:0') tensor(57171, device='cuda:0')\n",
            "(tensor(10.9598, device='cuda:0'), tensor(63899, device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_val_dataset_loss(dataset, model, tokenizer, val_frac = 0.2):\n",
        "    n = len(dataset)\n",
        "    val_size = int(n * val_frac)\n",
        "    return compute_dataset_loss(dataset[-val_size:], model, tokenizer)\n",
        "  \n",
        "# Compute validation loss\n",
        "# print(compute_val_dataset_loss(dataset, 0.1))"
      ],
      "metadata": {
        "id": "YR9C1TtTrLRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
        "\n",
        "def train_model(dataset, optimizer, epochs, model, tokenizer):\n",
        "    loss = 0\n",
        "    samples = 0\n",
        "    n = len(dataset)\n",
        "    batch_size = 2\n",
        "    batches = n // batch_size\n",
        "\n",
        "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Starting epoch: \", epoch)\n",
        "        for i in range(batches):\n",
        "            print(i, batch_size, loss, samples)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch = dataset[i:i+batch_size]\n",
        "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
        "            logits = model(**encoded_input)\n",
        "\n",
        "            # Find true labels and compute loss\n",
        "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
        "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
        "            samples = samples + valid_samples\n",
        "\n",
        "            # Backprop\n",
        "            ce_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    return loss, samples\n",
        "\n",
        "epochs = 1\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import random\n",
        "\n",
        "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
        "\n",
        "for i in range(4):\n",
        "  start_idx = 2000*i\n",
        "  end_idx = 2000*(i+1)\n",
        "  optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
        "  print(train_model(dataset[start_idx:end_idx], optimizer, epochs, simpleGPT2, tokenizer))\n",
        "# Loss reaches 6.3 after 3 epochs\n"
      ],
      "metadata": {
        "id": "J0e2dLg9rNJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generations are still nonsense\n",
        "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_OJ44_-_Um8",
        "outputId": "883833a6-de28-4be4-bb85-ff270b21fb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting prompt: Mary is the greatest. Or is she?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-65651643be1d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = t.nn.functional.softmax(logits)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current generation: Mary is the greatest. Or is she?, has,, and's and in,�,'s,,. ( and in's.\n",
            "ed and of and, in is and is was, in and,,,- and's's and.,, is and is, in. and, has of and, is and,�,, is's is and,,'s, and and ( is,, in,, of and, and (. in-,'s is, and,,, of.'s,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compute_val_dataset_loss(dataset, simpleGPT2, tokenizer, 0.1))"
      ],
      "metadata": {
        "id": "upDfqsth7ACE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "f8fd8eab-3d05-4045-d7f8-34de2b62ccbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 0 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b0795fe7b19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_val_dataset_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimpleGPT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-95f421039d3b>\u001b[0m in \u001b[0;36mcompute_val_dataset_loss\u001b[0;34m(dataset, model, tokenizer, val_frac)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_dataset_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Compute validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a6def445b98f>\u001b[0m in \u001b[0;36mcompute_dataset_loss\u001b[0;34m(dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;31m# Find true labels and compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mce_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalid_samples\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalid_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalid_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a6def445b98f>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(logits, encoded_input)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_samples_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_samples_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_samples_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_samples_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_dataset_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.92 GiB (GPU 0; 14.75 GiB total capacity; 10.22 GiB already allocated; 1.90 GiB free; 11.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgpgzU0RWOzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}