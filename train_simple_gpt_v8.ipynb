{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eed89d3-3348-46d6-a786-90ebb4017feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m191.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 regex-2023.3.23 tokenizers-0.13.3 transformers-4.27.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from datasets) (1.23.4)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./.local/lib/python3.8/site-packages (from datasets) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, urllib3, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, responses, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.3.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 urllib3-1.26.15 xxhash-3.2.0 yarl-1.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43aacda-f79c-4bdd-97ce-030ae0499c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4564fe0790bb416ca23c45b93a8c99b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d47cd5af6244ef953b362e7cb6dc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ae2a37ea66491dbb7c5ef16cad8908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6131efad6ff74f90a186ee1f97d5030a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c280b4885434d9f677b783376e2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fa6fd8-d3d0-4dbc-be81-e531a5441970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b60ccdcf9e4a3c9868be9c8b790a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9549b0b99e5c429ba1cee76e4a881c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3470aa57a747199cbc98fcda0580c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset openwebtext-10k/plain_text (download: 14.04 MiB, generated: 47.37 MiB, post-processed: Unknown size, total: 61.41 MiB) to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ed80a538af4e6cbaedb1fe6236bb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/download/download_manager.py:527: FutureWarning: 'num_proc' was deprecated in version 2.6.2 and will be removed in 3.0.0. Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19958db5b913410f97b110342bddb445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openwebtext-10k downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798b69c35e2b4e3d9525338fb686dbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700c4c5a-f2b9-4e1c-9d7d-5fa0965a73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bac5cbb-245c-473b-a5c0-25394f1568b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396d59ee-132c-4bd0-9a13-0c5776d0cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33624b5b-64ba-45d3-97b7-2875ef37482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072ba405-152a-44d1-b854-22cb180f8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
      "torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "simpleGPT2 = t.load('simpleGPT_55_epochs.pkl')\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3518804f-6a4e-4c3e-b639-d06846eb5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120560209\n"
     ]
    }
   ],
   "source": [
    "# How many parameters?\n",
    "print(sum((p.numel() if p.requires_grad else 0 for p in simpleGPT2.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f22e1960-82f0-4ad8-8172-7c09bf0f9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(21, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoded_input_alt = tokenizer(dataset[0][:100], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input_alt['attention_mask'].shape, encoded_input_alt['attention_mask'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b894e93f-04dc-42dc-9fa9-f54c88103b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "  return logits.argmax()\n",
    "\n",
    "def test_model(model, text = \"Replace me by any text you'd like.\", steps = 100, sampling = greedy_sampling, is_hf = False):\n",
    "    eos_token = \"<|endoftext|>\"\n",
    "    prompt = text\n",
    "    print(\"Starting prompt: \" + prompt)\n",
    "\n",
    "    for i in range(steps):\n",
    "        encoded_input = tokenizer([prompt], return_tensors=\"pt\", padding='max_length').to(device)\n",
    "        last_input_idx = encoded_input['attention_mask'][0].sum() - 1\n",
    "        if is_hf:\n",
    "            logits = model(**encoded_input).logits[0, last_input_idx]\n",
    "        else:\n",
    "            logits = model(**encoded_input)[0, last_input_idx]\n",
    "        next_token = sampling(logits)\n",
    "        next_string = tokenizer.decode(next_token)\n",
    "        if next_string == eos_token:\n",
    "            break\n",
    "        prompt = prompt + next_string\n",
    "    print(\"Current generation: \" + prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5d6e6d-7c3b-4849-9571-bbf505043c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b49424ce377b>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she?\n",
      "However, the other hand to the video game of some players just the show. This is a ranking for the last week where they have made away when I was working. I missed more than anything but I was just as possible.\n",
      "It was still fun but it I played by the same version and I am sitting on my life around-a. Here so, I ran at the first level, and I grew up until moving. I took some of it wasn't updating the number, I\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she? No, I think it is obvious. But, as a politician, I cannot just say things like that. I have to defend my right to say those things for a living with my life every day.\n",
      "\n",
      "\n",
      "It comes down to you as a person on this show. What is it about you that makes you tick? Is it because you like getting laughed at? Yes, it is. It's some of the things that make me tick, like who I am and what position. I\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(k):\n",
    "\n",
    "\n",
    "      def top_sampling(logits):\n",
    "          probs = t.nn.functional.softmax(logits)\n",
    "          values, indices = t.topk(probs, k)\n",
    "          index = values.multinomial(num_samples = 1, replacement = True)\n",
    "          return indices[index]\n",
    "      \n",
    "      return top_sampling\n",
    "\n",
    "# Our model generates English, but not really coherent generations.\n",
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fde2a27-c08a-4a3c-bfaa-deadcc89a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 0 0\n",
      "1 10 tensor(4.8142, device='cuda:0') tensor(6508, device='cuda:0')\n",
      "2 10 tensor(4.7791, device='cuda:0') tensor(12338, device='cuda:0')\n",
      "3 10 tensor(4.7999, device='cuda:0') tensor(18466, device='cuda:0')\n",
      "4 10 tensor(4.7994, device='cuda:0') tensor(24853, device='cuda:0')\n",
      "5 10 tensor(4.8089, device='cuda:0') tensor(30959, device='cuda:0')\n",
      "6 10 tensor(4.8076, device='cuda:0') tensor(37244, device='cuda:0')\n",
      "7 10 tensor(4.7791, device='cuda:0') tensor(43529, device='cuda:0')\n",
      "8 10 tensor(4.7568, device='cuda:0') tensor(50599, device='cuda:0')\n",
      "9 10 tensor(4.7367, device='cuda:0') tensor(57171, device='cuda:0')\n",
      "(tensor(4.7240, device='cuda:0'), tensor(63899, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batch_size = 10\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples\n",
    "\n",
    "# Compute loss of the pre-trained model on the truncated dataset\n",
    "print(compute_dataset_loss(dataset[:100], simpleGPT2, tokenizer))\n",
    "\n",
    "# Initial loss is 4.7, same as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de997d79-36e8-462f-84fb-742b2cd7c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_dataset_loss(dataset, model, tokenizer, val_frac = 0.2):\n",
    "    n = len(dataset)\n",
    "    val_size = int(n * val_frac)\n",
    "    return compute_dataset_loss(dataset[-val_size:], model, tokenizer)\n",
    "  \n",
    "# Compute validation loss\n",
    "# print(compute_val_dataset_loss(dataset, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402e6ee2-d473-4426-b171-16598bea8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "200 0.05 2 tensor(4.4114, device='cuda:0', grad_fn=<DivBackward0>) tensor(243865, device='cuda:0')\n",
      "400 0.1 2 tensor(4.3975, device='cuda:0', grad_fn=<DivBackward0>) tensor(488984, device='cuda:0')\n",
      "600 0.15 2 tensor(4.4067, device='cuda:0', grad_fn=<DivBackward0>) tensor(730605, device='cuda:0')\n",
      "800 0.2 2 tensor(4.4004, device='cuda:0', grad_fn=<DivBackward0>) tensor(963939, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.4078, device='cuda:0', grad_fn=<DivBackward0>) tensor(1210963, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.4116, device='cuda:0', grad_fn=<DivBackward0>) tensor(1443920, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.4265, device='cuda:0', grad_fn=<DivBackward0>) tensor(1676186, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.4307, device='cuda:0', grad_fn=<DivBackward0>) tensor(1925355, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.4370, device='cuda:0', grad_fn=<DivBackward0>) tensor(2173275, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.4399, device='cuda:0', grad_fn=<DivBackward0>) tensor(2406626, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.4499, device='cuda:0', grad_fn=<DivBackward0>) tensor(2655837, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.4495, device='cuda:0', grad_fn=<DivBackward0>) tensor(2888207, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.4490, device='cuda:0', grad_fn=<DivBackward0>) tensor(3119656, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.4495, device='cuda:0', grad_fn=<DivBackward0>) tensor(3368975, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.4517, device='cuda:0', grad_fn=<DivBackward0>) tensor(3588003, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.4540, device='cuda:0', grad_fn=<DivBackward0>) tensor(3827073, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.4564, device='cuda:0', grad_fn=<DivBackward0>) tensor(4070550, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.4584, device='cuda:0', grad_fn=<DivBackward0>) tensor(4303833, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.4603, device='cuda:0', grad_fn=<DivBackward0>) tensor(4542107, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(4.4651, device='cuda:0', grad_fn=<DivBackward0>) tensor(4795402, device='cuda:0')\n",
      "200 0.05 2 tensor(4.4624, device='cuda:0', grad_fn=<DivBackward0>) tensor(5022123, device='cuda:0')\n",
      "400 0.1 2 tensor(4.4649, device='cuda:0', grad_fn=<DivBackward0>) tensor(5273372, device='cuda:0')\n",
      "600 0.15 2 tensor(4.4671, device='cuda:0', grad_fn=<DivBackward0>) tensor(5517822, device='cuda:0')\n",
      "800 0.2 2 tensor(4.4657, device='cuda:0', grad_fn=<DivBackward0>) tensor(5748120, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.4676, device='cuda:0', grad_fn=<DivBackward0>) tensor(5994967, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.4708, device='cuda:0', grad_fn=<DivBackward0>) tensor(6242486, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.4715, device='cuda:0', grad_fn=<DivBackward0>) tensor(6487307, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.4758, device='cuda:0', grad_fn=<DivBackward0>) tensor(6735190, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.4736, device='cuda:0', grad_fn=<DivBackward0>) tensor(6966493, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.4716, device='cuda:0', grad_fn=<DivBackward0>) tensor(7199733, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.4730, device='cuda:0', grad_fn=<DivBackward0>) tensor(7446533, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.4728, device='cuda:0', grad_fn=<DivBackward0>) tensor(7691842, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.4750, device='cuda:0', grad_fn=<DivBackward0>) tensor(7925094, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.4748, device='cuda:0', grad_fn=<DivBackward0>) tensor(8165719, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.4767, device='cuda:0', grad_fn=<DivBackward0>) tensor(8414314, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.4785, device='cuda:0', grad_fn=<DivBackward0>) tensor(8647785, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.4803, device='cuda:0', grad_fn=<DivBackward0>) tensor(8895511, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.4823, device='cuda:0', grad_fn=<DivBackward0>) tensor(9144049, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.4854, device='cuda:0', grad_fn=<DivBackward0>) tensor(9385132, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(4.4863, device='cuda:0', grad_fn=<DivBackward0>) tensor(9627557, device='cuda:0')\n",
      "200 0.05 2 tensor(4.4870, device='cuda:0', grad_fn=<DivBackward0>) tensor(9870632, device='cuda:0')\n",
      "400 0.1 2 tensor(4.4877, device='cuda:0', grad_fn=<DivBackward0>) tensor(10114610, device='cuda:0')\n",
      "600 0.15 2 tensor(4.4877, device='cuda:0', grad_fn=<DivBackward0>) tensor(10348872, device='cuda:0')\n",
      "800 0.2 2 tensor(4.4889, device='cuda:0', grad_fn=<DivBackward0>) tensor(10591356, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.4894, device='cuda:0', grad_fn=<DivBackward0>) tensor(10826870, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.4919, device='cuda:0', grad_fn=<DivBackward0>) tensor(11080320, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.4925, device='cuda:0', grad_fn=<DivBackward0>) tensor(11326527, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.4922, device='cuda:0', grad_fn=<DivBackward0>) tensor(11557233, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.4933, device='cuda:0', grad_fn=<DivBackward0>) tensor(11790327, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.4938, device='cuda:0', grad_fn=<DivBackward0>) tensor(12029977, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.4965, device='cuda:0', grad_fn=<DivBackward0>) tensor(12285383, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.4984, device='cuda:0', grad_fn=<DivBackward0>) tensor(12524867, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.5014, device='cuda:0', grad_fn=<DivBackward0>) tensor(12771597, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.5016, device='cuda:0', grad_fn=<DivBackward0>) tensor(13006681, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.5036, device='cuda:0', grad_fn=<DivBackward0>) tensor(13255771, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.5060, device='cuda:0', grad_fn=<DivBackward0>) tensor(13502788, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.5077, device='cuda:0', grad_fn=<DivBackward0>) tensor(13746104, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.5107, device='cuda:0', grad_fn=<DivBackward0>) tensor(13993084, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.5124, device='cuda:0', grad_fn=<DivBackward0>) tensor(14230409, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(4.5146, device='cuda:0', grad_fn=<DivBackward0>) tensor(14469114, device='cuda:0')\n",
      "200 0.05 2 tensor(4.5152, device='cuda:0', grad_fn=<DivBackward0>) tensor(14709068, device='cuda:0')\n",
      "400 0.1 2 tensor(4.5172, device='cuda:0', grad_fn=<DivBackward0>) tensor(14955444, device='cuda:0')\n",
      "600 0.15 2 tensor(4.5192, device='cuda:0', grad_fn=<DivBackward0>) tensor(15213449, device='cuda:0')\n",
      "800 0.2 2 tensor(4.5212, device='cuda:0', grad_fn=<DivBackward0>) tensor(15454841, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.5229, device='cuda:0', grad_fn=<DivBackward0>) tensor(15694890, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.5241, device='cuda:0', grad_fn=<DivBackward0>) tensor(15928269, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.5262, device='cuda:0', grad_fn=<DivBackward0>) tensor(16168943, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.5277, device='cuda:0', grad_fn=<DivBackward0>) tensor(16411728, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.5280, device='cuda:0', grad_fn=<DivBackward0>) tensor(16640637, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.5293, device='cuda:0', grad_fn=<DivBackward0>) tensor(16877191, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.5321, device='cuda:0', grad_fn=<DivBackward0>) tensor(17122792, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.5348, device='cuda:0', grad_fn=<DivBackward0>) tensor(17375038, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.5367, device='cuda:0', grad_fn=<DivBackward0>) tensor(17608114, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.5387, device='cuda:0', grad_fn=<DivBackward0>) tensor(17847805, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.5418, device='cuda:0', grad_fn=<DivBackward0>) tensor(18094537, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.5444, device='cuda:0', grad_fn=<DivBackward0>) tensor(18338927, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.5472, device='cuda:0', grad_fn=<DivBackward0>) tensor(18586890, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.5501, device='cuda:0', grad_fn=<DivBackward0>) tensor(18831699, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.5527, device='cuda:0', grad_fn=<DivBackward0>) tensor(19072959, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(4.5555, device='cuda:0', grad_fn=<DivBackward0>) tensor(19316624, device='cuda:0')\n",
      "200 0.05 2 tensor(4.5557, device='cuda:0', grad_fn=<DivBackward0>) tensor(19558886, device='cuda:0')\n",
      "400 0.1 2 tensor(4.5563, device='cuda:0', grad_fn=<DivBackward0>) tensor(19800851, device='cuda:0')\n",
      "600 0.15 2 tensor(4.5572, device='cuda:0', grad_fn=<DivBackward0>) tensor(20036779, device='cuda:0')\n",
      "800 0.2 2 tensor(4.5579, device='cuda:0', grad_fn=<DivBackward0>) tensor(20274012, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.5592, device='cuda:0', grad_fn=<DivBackward0>) tensor(20515033, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.5624, device='cuda:0', grad_fn=<DivBackward0>) tensor(20754399, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.5646, device='cuda:0', grad_fn=<DivBackward0>) tensor(21009095, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.5668, device='cuda:0', grad_fn=<DivBackward0>) tensor(21246403, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.5691, device='cuda:0', grad_fn=<DivBackward0>) tensor(21492703, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.5710, device='cuda:0', grad_fn=<DivBackward0>) tensor(21726648, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.5729, device='cuda:0', grad_fn=<DivBackward0>) tensor(21963209, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.5747, device='cuda:0', grad_fn=<DivBackward0>) tensor(22198519, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.5765, device='cuda:0', grad_fn=<DivBackward0>) tensor(22434814, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.5793, device='cuda:0', grad_fn=<DivBackward0>) tensor(22685979, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.5817, device='cuda:0', grad_fn=<DivBackward0>) tensor(22929392, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.5846, device='cuda:0', grad_fn=<DivBackward0>) tensor(23167661, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.5873, device='cuda:0', grad_fn=<DivBackward0>) tensor(23419721, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.5898, device='cuda:0', grad_fn=<DivBackward0>) tensor(23658366, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.5923, device='cuda:0', grad_fn=<DivBackward0>) tensor(23901697, device='cuda:0')\n",
      "Starting epoch:  5\n",
      "0 0.0 2 tensor(4.5948, device='cuda:0', grad_fn=<DivBackward0>) tensor(24143653, device='cuda:0')\n",
      "200 0.05 2 tensor(4.5960, device='cuda:0', grad_fn=<DivBackward0>) tensor(24396738, device='cuda:0')\n",
      "400 0.1 2 tensor(4.5971, device='cuda:0', grad_fn=<DivBackward0>) tensor(24651762, device='cuda:0')\n",
      "600 0.15 2 tensor(4.5983, device='cuda:0', grad_fn=<DivBackward0>) tensor(24889419, device='cuda:0')\n",
      "800 0.2 2 tensor(4.5997, device='cuda:0', grad_fn=<DivBackward0>) tensor(25131670, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6017, device='cuda:0', grad_fn=<DivBackward0>) tensor(25384970, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6043, device='cuda:0', grad_fn=<DivBackward0>) tensor(25629759, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6061, device='cuda:0', grad_fn=<DivBackward0>) tensor(25874745, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6082, device='cuda:0', grad_fn=<DivBackward0>) tensor(26124359, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6101, device='cuda:0', grad_fn=<DivBackward0>) tensor(26367277, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6123, device='cuda:0', grad_fn=<DivBackward0>) tensor(26612755, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6138, device='cuda:0', grad_fn=<DivBackward0>) tensor(26842381, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6157, device='cuda:0', grad_fn=<DivBackward0>) tensor(27078951, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6182, device='cuda:0', grad_fn=<DivBackward0>) tensor(27319857, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6205, device='cuda:0', grad_fn=<DivBackward0>) tensor(27563173, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6231, device='cuda:0', grad_fn=<DivBackward0>) tensor(27819700, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6248, device='cuda:0', grad_fn=<DivBackward0>) tensor(28060338, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6272, device='cuda:0', grad_fn=<DivBackward0>) tensor(28313261, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6291, device='cuda:0', grad_fn=<DivBackward0>) tensor(28543159, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6310, device='cuda:0', grad_fn=<DivBackward0>) tensor(28786365, device='cuda:0')\n",
      "Starting epoch:  6\n",
      "0 0.0 2 tensor(4.6337, device='cuda:0', grad_fn=<DivBackward0>) tensor(29036330, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6345, device='cuda:0', grad_fn=<DivBackward0>) tensor(29281807, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6353, device='cuda:0', grad_fn=<DivBackward0>) tensor(29516680, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6356, device='cuda:0', grad_fn=<DivBackward0>) tensor(29752982, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6362, device='cuda:0', grad_fn=<DivBackward0>) tensor(29988604, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6371, device='cuda:0', grad_fn=<DivBackward0>) tensor(30220813, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6385, device='cuda:0', grad_fn=<DivBackward0>) tensor(30466020, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6407, device='cuda:0', grad_fn=<DivBackward0>) tensor(30719120, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6421, device='cuda:0', grad_fn=<DivBackward0>) tensor(30962854, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6437, device='cuda:0', grad_fn=<DivBackward0>) tensor(31202736, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6445, device='cuda:0', grad_fn=<DivBackward0>) tensor(31433900, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6460, device='cuda:0', grad_fn=<DivBackward0>) tensor(31673876, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6478, device='cuda:0', grad_fn=<DivBackward0>) tensor(31912099, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6494, device='cuda:0', grad_fn=<DivBackward0>) tensor(32162365, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6515, device='cuda:0', grad_fn=<DivBackward0>) tensor(32407899, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6537, device='cuda:0', grad_fn=<DivBackward0>) tensor(32652103, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6551, device='cuda:0', grad_fn=<DivBackward0>) tensor(32886770, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6570, device='cuda:0', grad_fn=<DivBackward0>) tensor(33127677, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6587, device='cuda:0', grad_fn=<DivBackward0>) tensor(33364867, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6602, device='cuda:0', grad_fn=<DivBackward0>) tensor(33596287, device='cuda:0')\n",
      "Starting epoch:  7\n",
      "0 0.0 2 tensor(4.6629, device='cuda:0', grad_fn=<DivBackward0>) tensor(33852855, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6635, device='cuda:0', grad_fn=<DivBackward0>) tensor(34104887, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6637, device='cuda:0', grad_fn=<DivBackward0>) tensor(34342534, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6638, device='cuda:0', grad_fn=<DivBackward0>) tensor(34577313, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6648, device='cuda:0', grad_fn=<DivBackward0>) tensor(34828131, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6655, device='cuda:0', grad_fn=<DivBackward0>) tensor(35061250, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6660, device='cuda:0', grad_fn=<DivBackward0>) tensor(35297515, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6665, device='cuda:0', grad_fn=<DivBackward0>) tensor(35532591, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6676, device='cuda:0', grad_fn=<DivBackward0>) tensor(35774729, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6687, device='cuda:0', grad_fn=<DivBackward0>) tensor(36015780, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6695, device='cuda:0', grad_fn=<DivBackward0>) tensor(36251255, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6706, device='cuda:0', grad_fn=<DivBackward0>) tensor(36486372, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6718, device='cuda:0', grad_fn=<DivBackward0>) tensor(36728366, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6734, device='cuda:0', grad_fn=<DivBackward0>) tensor(36973796, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6740, device='cuda:0', grad_fn=<DivBackward0>) tensor(37204359, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6748, device='cuda:0', grad_fn=<DivBackward0>) tensor(37448885, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6759, device='cuda:0', grad_fn=<DivBackward0>) tensor(37684403, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6775, device='cuda:0', grad_fn=<DivBackward0>) tensor(37931303, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6794, device='cuda:0', grad_fn=<DivBackward0>) tensor(38167204, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6808, device='cuda:0', grad_fn=<DivBackward0>) tensor(38410908, device='cuda:0')\n",
      "Starting epoch:  8\n",
      "0 0.0 2 tensor(4.6825, device='cuda:0', grad_fn=<DivBackward0>) tensor(38661904, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6830, device='cuda:0', grad_fn=<DivBackward0>) tensor(38902662, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6838, device='cuda:0', grad_fn=<DivBackward0>) tensor(39153841, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6840, device='cuda:0', grad_fn=<DivBackward0>) tensor(39390757, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6846, device='cuda:0', grad_fn=<DivBackward0>) tensor(39626090, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6848, device='cuda:0', grad_fn=<DivBackward0>) tensor(39872818, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6853, device='cuda:0', grad_fn=<DivBackward0>) tensor(40106974, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6863, device='cuda:0', grad_fn=<DivBackward0>) tensor(40358596, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6868, device='cuda:0', grad_fn=<DivBackward0>) tensor(40589144, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6878, device='cuda:0', grad_fn=<DivBackward0>) tensor(40837479, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6891, device='cuda:0', grad_fn=<DivBackward0>) tensor(41086066, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6896, device='cuda:0', grad_fn=<DivBackward0>) tensor(41328976, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6904, device='cuda:0', grad_fn=<DivBackward0>) tensor(41571275, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6916, device='cuda:0', grad_fn=<DivBackward0>) tensor(41822102, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6925, device='cuda:0', grad_fn=<DivBackward0>) tensor(42060691, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6937, device='cuda:0', grad_fn=<DivBackward0>) tensor(42309536, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6949, device='cuda:0', grad_fn=<DivBackward0>) tensor(42552362, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6962, device='cuda:0', grad_fn=<DivBackward0>) tensor(42800106, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6972, device='cuda:0', grad_fn=<DivBackward0>) tensor(43030584, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6986, device='cuda:0', grad_fn=<DivBackward0>) tensor(43272544, device='cuda:0')\n",
      "Starting epoch:  9\n",
      "0 0.0 2 tensor(4.6997, device='cuda:0', grad_fn=<DivBackward0>) tensor(43511122, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6997, device='cuda:0', grad_fn=<DivBackward0>) tensor(43747396, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6997, device='cuda:0', grad_fn=<DivBackward0>) tensor(43977353, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6997, device='cuda:0', grad_fn=<DivBackward0>) tensor(44222272, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7000, device='cuda:0', grad_fn=<DivBackward0>) tensor(44471639, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7005, device='cuda:0', grad_fn=<DivBackward0>) tensor(44719899, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7008, device='cuda:0', grad_fn=<DivBackward0>) tensor(44956114, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7015, device='cuda:0', grad_fn=<DivBackward0>) tensor(45209786, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7020, device='cuda:0', grad_fn=<DivBackward0>) tensor(45459289, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7025, device='cuda:0', grad_fn=<DivBackward0>) tensor(45696629, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7030, device='cuda:0', grad_fn=<DivBackward0>) tensor(45937792, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7037, device='cuda:0', grad_fn=<DivBackward0>) tensor(46186376, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7040, device='cuda:0', grad_fn=<DivBackward0>) tensor(46429452, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7047, device='cuda:0', grad_fn=<DivBackward0>) tensor(46663339, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7056, device='cuda:0', grad_fn=<DivBackward0>) tensor(46910194, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7062, device='cuda:0', grad_fn=<DivBackward0>) tensor(47142854, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7068, device='cuda:0', grad_fn=<DivBackward0>) tensor(47385414, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7074, device='cuda:0', grad_fn=<DivBackward0>) tensor(47626129, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7085, device='cuda:0', grad_fn=<DivBackward0>) tensor(47866311, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7091, device='cuda:0', grad_fn=<DivBackward0>) tensor(48105995, device='cuda:0')\n",
      "Starting epoch:  10\n",
      "0 0.0 2 tensor(4.7100, device='cuda:0', grad_fn=<DivBackward0>) tensor(48359078, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7098, device='cuda:0', grad_fn=<DivBackward0>) tensor(48592480, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7099, device='cuda:0', grad_fn=<DivBackward0>) tensor(48852530, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7103, device='cuda:0', grad_fn=<DivBackward0>) tensor(49096962, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7100, device='cuda:0', grad_fn=<DivBackward0>) tensor(49327186, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7106, device='cuda:0', grad_fn=<DivBackward0>) tensor(49579277, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7108, device='cuda:0', grad_fn=<DivBackward0>) tensor(49824411, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7113, device='cuda:0', grad_fn=<DivBackward0>) tensor(50058750, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7116, device='cuda:0', grad_fn=<DivBackward0>) tensor(50299337, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7115, device='cuda:0', grad_fn=<DivBackward0>) tensor(50534091, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7120, device='cuda:0', grad_fn=<DivBackward0>) tensor(50783905, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7125, device='cuda:0', grad_fn=<DivBackward0>) tensor(51028410, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7131, device='cuda:0', grad_fn=<DivBackward0>) tensor(51274730, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7138, device='cuda:0', grad_fn=<DivBackward0>) tensor(51522064, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7145, device='cuda:0', grad_fn=<DivBackward0>) tensor(51772620, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7148, device='cuda:0', grad_fn=<DivBackward0>) tensor(52004076, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7155, device='cuda:0', grad_fn=<DivBackward0>) tensor(52243217, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7161, device='cuda:0', grad_fn=<DivBackward0>) tensor(52489331, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7171, device='cuda:0', grad_fn=<DivBackward0>) tensor(52731184, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7177, device='cuda:0', grad_fn=<DivBackward0>) tensor(52975886, device='cuda:0')\n",
      "Starting epoch:  11\n",
      "0 0.0 2 tensor(4.7182, device='cuda:0', grad_fn=<DivBackward0>) tensor(53214176, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7179, device='cuda:0', grad_fn=<DivBackward0>) tensor(53452439, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7176, device='cuda:0', grad_fn=<DivBackward0>) tensor(53694235, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7173, device='cuda:0', grad_fn=<DivBackward0>) tensor(53932881, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7171, device='cuda:0', grad_fn=<DivBackward0>) tensor(54176787, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7168, device='cuda:0', grad_fn=<DivBackward0>) tensor(54427847, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7172, device='cuda:0', grad_fn=<DivBackward0>) tensor(54674265, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7175, device='cuda:0', grad_fn=<DivBackward0>) tensor(54919526, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7178, device='cuda:0', grad_fn=<DivBackward0>) tensor(55149193, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7181, device='cuda:0', grad_fn=<DivBackward0>) tensor(55390005, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7183, device='cuda:0', grad_fn=<DivBackward0>) tensor(55634569, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7188, device='cuda:0', grad_fn=<DivBackward0>) tensor(55878653, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7191, device='cuda:0', grad_fn=<DivBackward0>) tensor(56115493, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7197, device='cuda:0', grad_fn=<DivBackward0>) tensor(56364845, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7201, device='cuda:0', grad_fn=<DivBackward0>) tensor(56610356, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7202, device='cuda:0', grad_fn=<DivBackward0>) tensor(56853585, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7206, device='cuda:0', grad_fn=<DivBackward0>) tensor(57087084, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7212, device='cuda:0', grad_fn=<DivBackward0>) tensor(57335544, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7219, device='cuda:0', grad_fn=<DivBackward0>) tensor(57583032, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7224, device='cuda:0', grad_fn=<DivBackward0>) tensor(57818648, device='cuda:0')\n",
      "Starting epoch:  12\n",
      "0 0.0 2 tensor(4.7225, device='cuda:0', grad_fn=<DivBackward0>) tensor(58050436, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(58269307, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7216, device='cuda:0', grad_fn=<DivBackward0>) tensor(58512604, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7214, device='cuda:0', grad_fn=<DivBackward0>) tensor(58759120, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7214, device='cuda:0', grad_fn=<DivBackward0>) tensor(59001979, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7213, device='cuda:0', grad_fn=<DivBackward0>) tensor(59245216, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7212, device='cuda:0', grad_fn=<DivBackward0>) tensor(59482308, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7209, device='cuda:0', grad_fn=<DivBackward0>) tensor(59730975, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7211, device='cuda:0', grad_fn=<DivBackward0>) tensor(59975433, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7211, device='cuda:0', grad_fn=<DivBackward0>) tensor(60220677, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7211, device='cuda:0', grad_fn=<DivBackward0>) tensor(60460621, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7215, device='cuda:0', grad_fn=<DivBackward0>) tensor(60702036, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7216, device='cuda:0', grad_fn=<DivBackward0>) tensor(60948493, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(61187891, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7222, device='cuda:0', grad_fn=<DivBackward0>) tensor(61423290, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7223, device='cuda:0', grad_fn=<DivBackward0>) tensor(61659539, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7228, device='cuda:0', grad_fn=<DivBackward0>) tensor(61904218, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7232, device='cuda:0', grad_fn=<DivBackward0>) tensor(62145819, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7234, device='cuda:0', grad_fn=<DivBackward0>) tensor(62394802, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7235, device='cuda:0', grad_fn=<DivBackward0>) tensor(62636609, device='cuda:0')\n",
      "Starting epoch:  13\n",
      "0 0.0 2 tensor(4.7235, device='cuda:0', grad_fn=<DivBackward0>) tensor(62868842, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7228, device='cuda:0', grad_fn=<DivBackward0>) tensor(63102036, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7226, device='cuda:0', grad_fn=<DivBackward0>) tensor(63350380, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7223, device='cuda:0', grad_fn=<DivBackward0>) tensor(63588855, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7223, device='cuda:0', grad_fn=<DivBackward0>) tensor(63848600, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7223, device='cuda:0', grad_fn=<DivBackward0>) tensor(64097250, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7222, device='cuda:0', grad_fn=<DivBackward0>) tensor(64351853, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(64584664, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(64828147, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(65063815, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7219, device='cuda:0', grad_fn=<DivBackward0>) tensor(65313594, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(65568385, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7219, device='cuda:0', grad_fn=<DivBackward0>) tensor(65790681, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7220, device='cuda:0', grad_fn=<DivBackward0>) tensor(66030093, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7222, device='cuda:0', grad_fn=<DivBackward0>) tensor(66270435, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7219, device='cuda:0', grad_fn=<DivBackward0>) tensor(66505339, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7223, device='cuda:0', grad_fn=<DivBackward0>) tensor(66762779, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7225, device='cuda:0', grad_fn=<DivBackward0>) tensor(67008062, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7227, device='cuda:0', grad_fn=<DivBackward0>) tensor(67248044, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7229, device='cuda:0', grad_fn=<DivBackward0>) tensor(67496239, device='cuda:0')\n",
      "Starting epoch:  14\n",
      "0 0.0 2 tensor(4.7231, device='cuda:0', grad_fn=<DivBackward0>) tensor(67731144, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7226, device='cuda:0', grad_fn=<DivBackward0>) tensor(67969808, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7221, device='cuda:0', grad_fn=<DivBackward0>) tensor(68213545, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7218, device='cuda:0', grad_fn=<DivBackward0>) tensor(68454515, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7215, device='cuda:0', grad_fn=<DivBackward0>) tensor(68694047, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7212, device='cuda:0', grad_fn=<DivBackward0>) tensor(68941159, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7209, device='cuda:0', grad_fn=<DivBackward0>) tensor(69180707, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7209, device='cuda:0', grad_fn=<DivBackward0>) tensor(69429632, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7210, device='cuda:0', grad_fn=<DivBackward0>) tensor(69677100, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7208, device='cuda:0', grad_fn=<DivBackward0>) tensor(69917650, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7207, device='cuda:0', grad_fn=<DivBackward0>) tensor(70156080, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7204, device='cuda:0', grad_fn=<DivBackward0>) tensor(70388246, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7204, device='cuda:0', grad_fn=<DivBackward0>) tensor(70634790, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7205, device='cuda:0', grad_fn=<DivBackward0>) tensor(70873773, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7203, device='cuda:0', grad_fn=<DivBackward0>) tensor(71114049, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7203, device='cuda:0', grad_fn=<DivBackward0>) tensor(71355024, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7203, device='cuda:0', grad_fn=<DivBackward0>) tensor(71600421, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7202, device='cuda:0', grad_fn=<DivBackward0>) tensor(71836605, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7202, device='cuda:0', grad_fn=<DivBackward0>) tensor(72063726, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7201, device='cuda:0', grad_fn=<DivBackward0>) tensor(72295901, device='cuda:0')\n",
      "Starting epoch:  15\n",
      "0 0.0 2 tensor(4.7203, device='cuda:0', grad_fn=<DivBackward0>) tensor(72543482, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7200, device='cuda:0', grad_fn=<DivBackward0>) tensor(72780405, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7196, device='cuda:0', grad_fn=<DivBackward0>) tensor(73022215, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7194, device='cuda:0', grad_fn=<DivBackward0>) tensor(73260296, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7188, device='cuda:0', grad_fn=<DivBackward0>) tensor(73503418, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7185, device='cuda:0', grad_fn=<DivBackward0>) tensor(73755723, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7183, device='cuda:0', grad_fn=<DivBackward0>) tensor(74003622, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7180, device='cuda:0', grad_fn=<DivBackward0>) tensor(74255783, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7178, device='cuda:0', grad_fn=<DivBackward0>) tensor(74504032, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7176, device='cuda:0', grad_fn=<DivBackward0>) tensor(74752263, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7173, device='cuda:0', grad_fn=<DivBackward0>) tensor(74992960, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7172, device='cuda:0', grad_fn=<DivBackward0>) tensor(75241565, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7169, device='cuda:0', grad_fn=<DivBackward0>) tensor(75485735, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7169, device='cuda:0', grad_fn=<DivBackward0>) tensor(75740528, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7167, device='cuda:0', grad_fn=<DivBackward0>) tensor(75976683, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7164, device='cuda:0', grad_fn=<DivBackward0>) tensor(76213252, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7162, device='cuda:0', grad_fn=<DivBackward0>) tensor(76452185, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7158, device='cuda:0', grad_fn=<DivBackward0>) tensor(76674617, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7156, device='cuda:0', grad_fn=<DivBackward0>) tensor(76901278, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7155, device='cuda:0', grad_fn=<DivBackward0>) tensor(77125678, device='cuda:0')\n",
      "Starting epoch:  16\n",
      "0 0.0 2 tensor(4.7155, device='cuda:0', grad_fn=<DivBackward0>) tensor(77364637, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7150, device='cuda:0', grad_fn=<DivBackward0>) tensor(77606883, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7145, device='cuda:0', grad_fn=<DivBackward0>) tensor(77856389, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7139, device='cuda:0', grad_fn=<DivBackward0>) tensor(78087913, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7132, device='cuda:0', grad_fn=<DivBackward0>) tensor(78321495, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7130, device='cuda:0', grad_fn=<DivBackward0>) tensor(78566544, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7126, device='cuda:0', grad_fn=<DivBackward0>) tensor(78814108, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7123, device='cuda:0', grad_fn=<DivBackward0>) tensor(79053978, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7119, device='cuda:0', grad_fn=<DivBackward0>) tensor(79288464, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7117, device='cuda:0', grad_fn=<DivBackward0>) tensor(79538352, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7111, device='cuda:0', grad_fn=<DivBackward0>) tensor(79775284, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7106, device='cuda:0', grad_fn=<DivBackward0>) tensor(80004808, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7103, device='cuda:0', grad_fn=<DivBackward0>) tensor(80244922, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7100, device='cuda:0', grad_fn=<DivBackward0>) tensor(80479729, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7097, device='cuda:0', grad_fn=<DivBackward0>) tensor(80723963, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7095, device='cuda:0', grad_fn=<DivBackward0>) tensor(80972001, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7094, device='cuda:0', grad_fn=<DivBackward0>) tensor(81227344, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7094, device='cuda:0', grad_fn=<DivBackward0>) tensor(81475802, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7092, device='cuda:0', grad_fn=<DivBackward0>) tensor(81717404, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7090, device='cuda:0', grad_fn=<DivBackward0>) tensor(81952743, device='cuda:0')\n",
      "Starting epoch:  17\n",
      "0 0.0 2 tensor(4.7084, device='cuda:0', grad_fn=<DivBackward0>) tensor(82186122, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7078, device='cuda:0', grad_fn=<DivBackward0>) tensor(82435899, device='cuda:0')\n",
      "400 0.1 2 tensor(4.7073, device='cuda:0', grad_fn=<DivBackward0>) tensor(82685208, device='cuda:0')\n",
      "600 0.15 2 tensor(4.7068, device='cuda:0', grad_fn=<DivBackward0>) tensor(82932329, device='cuda:0')\n",
      "800 0.2 2 tensor(4.7064, device='cuda:0', grad_fn=<DivBackward0>) tensor(83180937, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.7057, device='cuda:0', grad_fn=<DivBackward0>) tensor(83419195, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.7053, device='cuda:0', grad_fn=<DivBackward0>) tensor(83656665, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.7051, device='cuda:0', grad_fn=<DivBackward0>) tensor(83895121, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.7048, device='cuda:0', grad_fn=<DivBackward0>) tensor(84142619, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.7045, device='cuda:0', grad_fn=<DivBackward0>) tensor(84380234, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.7040, device='cuda:0', grad_fn=<DivBackward0>) tensor(84616586, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.7036, device='cuda:0', grad_fn=<DivBackward0>) tensor(84862980, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.7034, device='cuda:0', grad_fn=<DivBackward0>) tensor(85106068, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.7030, device='cuda:0', grad_fn=<DivBackward0>) tensor(85339874, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.7028, device='cuda:0', grad_fn=<DivBackward0>) tensor(85577740, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.7024, device='cuda:0', grad_fn=<DivBackward0>) tensor(85821645, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.7021, device='cuda:0', grad_fn=<DivBackward0>) tensor(86064330, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.7018, device='cuda:0', grad_fn=<DivBackward0>) tensor(86307445, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.7014, device='cuda:0', grad_fn=<DivBackward0>) tensor(86553373, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.7010, device='cuda:0', grad_fn=<DivBackward0>) tensor(86790169, device='cuda:0')\n",
      "Starting epoch:  18\n",
      "0 0.0 2 tensor(4.7007, device='cuda:0', grad_fn=<DivBackward0>) tensor(87023258, device='cuda:0')\n",
      "200 0.05 2 tensor(4.7000, device='cuda:0', grad_fn=<DivBackward0>) tensor(87250848, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6996, device='cuda:0', grad_fn=<DivBackward0>) tensor(87505742, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6989, device='cuda:0', grad_fn=<DivBackward0>) tensor(87744510, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6984, device='cuda:0', grad_fn=<DivBackward0>) tensor(87988270, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6978, device='cuda:0', grad_fn=<DivBackward0>) tensor(88224461, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6975, device='cuda:0', grad_fn=<DivBackward0>) tensor(88461162, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6969, device='cuda:0', grad_fn=<DivBackward0>) tensor(88698952, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6964, device='cuda:0', grad_fn=<DivBackward0>) tensor(88924546, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6958, device='cuda:0', grad_fn=<DivBackward0>) tensor(89158618, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6955, device='cuda:0', grad_fn=<DivBackward0>) tensor(89413498, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6951, device='cuda:0', grad_fn=<DivBackward0>) tensor(89651094, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6944, device='cuda:0', grad_fn=<DivBackward0>) tensor(89885113, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6942, device='cuda:0', grad_fn=<DivBackward0>) tensor(90123818, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6938, device='cuda:0', grad_fn=<DivBackward0>) tensor(90368042, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6935, device='cuda:0', grad_fn=<DivBackward0>) tensor(90604058, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6930, device='cuda:0', grad_fn=<DivBackward0>) tensor(90840045, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6927, device='cuda:0', grad_fn=<DivBackward0>) tensor(91091224, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6923, device='cuda:0', grad_fn=<DivBackward0>) tensor(91335336, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6919, device='cuda:0', grad_fn=<DivBackward0>) tensor(91575692, device='cuda:0')\n",
      "Starting epoch:  19\n",
      "0 0.0 2 tensor(4.6918, device='cuda:0', grad_fn=<DivBackward0>) tensor(91827375, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6912, device='cuda:0', grad_fn=<DivBackward0>) tensor(92079199, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6905, device='cuda:0', grad_fn=<DivBackward0>) tensor(92328890, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6900, device='cuda:0', grad_fn=<DivBackward0>) tensor(92578060, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6896, device='cuda:0', grad_fn=<DivBackward0>) tensor(92822835, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6892, device='cuda:0', grad_fn=<DivBackward0>) tensor(93072188, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6887, device='cuda:0', grad_fn=<DivBackward0>) tensor(93313389, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6882, device='cuda:0', grad_fn=<DivBackward0>) tensor(93553494, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6878, device='cuda:0', grad_fn=<DivBackward0>) tensor(93796965, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6873, device='cuda:0', grad_fn=<DivBackward0>) tensor(94040086, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6867, device='cuda:0', grad_fn=<DivBackward0>) tensor(94272698, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6864, device='cuda:0', grad_fn=<DivBackward0>) tensor(94524179, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6859, device='cuda:0', grad_fn=<DivBackward0>) tensor(94764514, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6854, device='cuda:0', grad_fn=<DivBackward0>) tensor(95001494, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6851, device='cuda:0', grad_fn=<DivBackward0>) tensor(95252110, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6848, device='cuda:0', grad_fn=<DivBackward0>) tensor(95497288, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6844, device='cuda:0', grad_fn=<DivBackward0>) tensor(95735470, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6839, device='cuda:0', grad_fn=<DivBackward0>) tensor(95974316, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6835, device='cuda:0', grad_fn=<DivBackward0>) tensor(96223562, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6833, device='cuda:0', grad_fn=<DivBackward0>) tensor(96464355, device='cuda:0')\n",
      "Starting epoch:  20\n",
      "0 0.0 2 tensor(4.6829, device='cuda:0', grad_fn=<DivBackward0>) tensor(96710574, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6823, device='cuda:0', grad_fn=<DivBackward0>) tensor(96956343, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6816, device='cuda:0', grad_fn=<DivBackward0>) tensor(97201859, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6812, device='cuda:0', grad_fn=<DivBackward0>) tensor(97446071, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6804, device='cuda:0', grad_fn=<DivBackward0>) tensor(97677448, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6798, device='cuda:0', grad_fn=<DivBackward0>) tensor(97918858, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6791, device='cuda:0', grad_fn=<DivBackward0>) tensor(98143582, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6784, device='cuda:0', grad_fn=<DivBackward0>) tensor(98377290, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6782, device='cuda:0', grad_fn=<DivBackward0>) tensor(98626759, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6776, device='cuda:0', grad_fn=<DivBackward0>) tensor(98861749, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6771, device='cuda:0', grad_fn=<DivBackward0>) tensor(99104562, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6768, device='cuda:0', grad_fn=<DivBackward0>) tensor(99349460, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6761, device='cuda:0', grad_fn=<DivBackward0>) tensor(99597914, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6757, device='cuda:0', grad_fn=<DivBackward0>) tensor(99842469, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6755, device='cuda:0', grad_fn=<DivBackward0>) tensor(100084330, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6749, device='cuda:0', grad_fn=<DivBackward0>) tensor(100326401, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6745, device='cuda:0', grad_fn=<DivBackward0>) tensor(100575134, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6740, device='cuda:0', grad_fn=<DivBackward0>) tensor(100824510, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6735, device='cuda:0', grad_fn=<DivBackward0>) tensor(101061417, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6731, device='cuda:0', grad_fn=<DivBackward0>) tensor(101307944, device='cuda:0')\n",
      "Starting epoch:  21\n",
      "0 0.0 2 tensor(4.6726, device='cuda:0', grad_fn=<DivBackward0>) tensor(101551072, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6718, device='cuda:0', grad_fn=<DivBackward0>) tensor(101784368, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6711, device='cuda:0', grad_fn=<DivBackward0>) tensor(102024553, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6705, device='cuda:0', grad_fn=<DivBackward0>) tensor(102266358, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6701, device='cuda:0', grad_fn=<DivBackward0>) tensor(102504451, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6696, device='cuda:0', grad_fn=<DivBackward0>) tensor(102752325, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6690, device='cuda:0', grad_fn=<DivBackward0>) tensor(103011224, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6683, device='cuda:0', grad_fn=<DivBackward0>) tensor(103242846, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6677, device='cuda:0', grad_fn=<DivBackward0>) tensor(103482119, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6672, device='cuda:0', grad_fn=<DivBackward0>) tensor(103738221, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6666, device='cuda:0', grad_fn=<DivBackward0>) tensor(103975252, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6659, device='cuda:0', grad_fn=<DivBackward0>) tensor(104210895, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6654, device='cuda:0', grad_fn=<DivBackward0>) tensor(104446277, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6649, device='cuda:0', grad_fn=<DivBackward0>) tensor(104686512, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6642, device='cuda:0', grad_fn=<DivBackward0>) tensor(104935663, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6637, device='cuda:0', grad_fn=<DivBackward0>) tensor(105171839, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6631, device='cuda:0', grad_fn=<DivBackward0>) tensor(105424723, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6628, device='cuda:0', grad_fn=<DivBackward0>) tensor(105670827, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6623, device='cuda:0', grad_fn=<DivBackward0>) tensor(105906652, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6620, device='cuda:0', grad_fn=<DivBackward0>) tensor(106156871, device='cuda:0')\n",
      "Starting epoch:  22\n",
      "0 0.0 2 tensor(4.6616, device='cuda:0', grad_fn=<DivBackward0>) tensor(106396510, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6610, device='cuda:0', grad_fn=<DivBackward0>) tensor(106642462, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6602, device='cuda:0', grad_fn=<DivBackward0>) tensor(106879793, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6597, device='cuda:0', grad_fn=<DivBackward0>) tensor(107128896, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6592, device='cuda:0', grad_fn=<DivBackward0>) tensor(107374706, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6586, device='cuda:0', grad_fn=<DivBackward0>) tensor(107609903, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6579, device='cuda:0', grad_fn=<DivBackward0>) tensor(107844856, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6574, device='cuda:0', grad_fn=<DivBackward0>) tensor(108095215, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6568, device='cuda:0', grad_fn=<DivBackward0>) tensor(108340924, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6563, device='cuda:0', grad_fn=<DivBackward0>) tensor(108585261, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6558, device='cuda:0', grad_fn=<DivBackward0>) tensor(108827024, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6551, device='cuda:0', grad_fn=<DivBackward0>) tensor(109060422, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6546, device='cuda:0', grad_fn=<DivBackward0>) tensor(109313591, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6540, device='cuda:0', grad_fn=<DivBackward0>) tensor(109557578, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6535, device='cuda:0', grad_fn=<DivBackward0>) tensor(109787564, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6531, device='cuda:0', grad_fn=<DivBackward0>) tensor(110035646, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6525, device='cuda:0', grad_fn=<DivBackward0>) tensor(110267305, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6518, device='cuda:0', grad_fn=<DivBackward0>) tensor(110508773, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6513, device='cuda:0', grad_fn=<DivBackward0>) tensor(110753345, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6509, device='cuda:0', grad_fn=<DivBackward0>) tensor(111005056, device='cuda:0')\n",
      "Starting epoch:  23\n",
      "0 0.0 2 tensor(4.6505, device='cuda:0', grad_fn=<DivBackward0>) tensor(111253877, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6497, device='cuda:0', grad_fn=<DivBackward0>) tensor(111481111, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6490, device='cuda:0', grad_fn=<DivBackward0>) tensor(111728098, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6483, device='cuda:0', grad_fn=<DivBackward0>) tensor(111966041, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6476, device='cuda:0', grad_fn=<DivBackward0>) tensor(112194430, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6471, device='cuda:0', grad_fn=<DivBackward0>) tensor(112436912, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6463, device='cuda:0', grad_fn=<DivBackward0>) tensor(112670002, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6456, device='cuda:0', grad_fn=<DivBackward0>) tensor(112904961, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6451, device='cuda:0', grad_fn=<DivBackward0>) tensor(113148167, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6445, device='cuda:0', grad_fn=<DivBackward0>) tensor(113399103, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6441, device='cuda:0', grad_fn=<DivBackward0>) tensor(113640166, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6435, device='cuda:0', grad_fn=<DivBackward0>) tensor(113893675, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6430, device='cuda:0', grad_fn=<DivBackward0>) tensor(114131670, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6426, device='cuda:0', grad_fn=<DivBackward0>) tensor(114389240, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6421, device='cuda:0', grad_fn=<DivBackward0>) tensor(114634312, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6416, device='cuda:0', grad_fn=<DivBackward0>) tensor(114880944, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6412, device='cuda:0', grad_fn=<DivBackward0>) tensor(115134426, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6406, device='cuda:0', grad_fn=<DivBackward0>) tensor(115366239, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6402, device='cuda:0', grad_fn=<DivBackward0>) tensor(115610843, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6397, device='cuda:0', grad_fn=<DivBackward0>) tensor(115849839, device='cuda:0')\n",
      "Starting epoch:  24\n",
      "0 0.0 2 tensor(4.6392, device='cuda:0', grad_fn=<DivBackward0>) tensor(116087332, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6386, device='cuda:0', grad_fn=<DivBackward0>) tensor(116322065, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6380, device='cuda:0', grad_fn=<DivBackward0>) tensor(116563612, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6374, device='cuda:0', grad_fn=<DivBackward0>) tensor(116803661, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6367, device='cuda:0', grad_fn=<DivBackward0>) tensor(117046324, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6361, device='cuda:0', grad_fn=<DivBackward0>) tensor(117288474, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6354, device='cuda:0', grad_fn=<DivBackward0>) tensor(117521818, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6347, device='cuda:0', grad_fn=<DivBackward0>) tensor(117763579, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6340, device='cuda:0', grad_fn=<DivBackward0>) tensor(117998977, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6335, device='cuda:0', grad_fn=<DivBackward0>) tensor(118243303, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6330, device='cuda:0', grad_fn=<DivBackward0>) tensor(118487175, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6324, device='cuda:0', grad_fn=<DivBackward0>) tensor(118727218, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6319, device='cuda:0', grad_fn=<DivBackward0>) tensor(118981698, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6313, device='cuda:0', grad_fn=<DivBackward0>) tensor(119229168, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6306, device='cuda:0', grad_fn=<DivBackward0>) tensor(119466926, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6299, device='cuda:0', grad_fn=<DivBackward0>) tensor(119712257, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6294, device='cuda:0', grad_fn=<DivBackward0>) tensor(119961568, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6289, device='cuda:0', grad_fn=<DivBackward0>) tensor(120205918, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6283, device='cuda:0', grad_fn=<DivBackward0>) tensor(120437854, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6278, device='cuda:0', grad_fn=<DivBackward0>) tensor(120673989, device='cuda:0')\n",
      "Starting epoch:  25\n",
      "0 0.0 2 tensor(4.6273, device='cuda:0', grad_fn=<DivBackward0>) tensor(120925471, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6266, device='cuda:0', grad_fn=<DivBackward0>) tensor(121163750, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6261, device='cuda:0', grad_fn=<DivBackward0>) tensor(121416107, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6256, device='cuda:0', grad_fn=<DivBackward0>) tensor(121659412, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6249, device='cuda:0', grad_fn=<DivBackward0>) tensor(121896390, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6244, device='cuda:0', grad_fn=<DivBackward0>) tensor(122137405, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6239, device='cuda:0', grad_fn=<DivBackward0>) tensor(122383177, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6233, device='cuda:0', grad_fn=<DivBackward0>) tensor(122628825, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6227, device='cuda:0', grad_fn=<DivBackward0>) tensor(122872756, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6223, device='cuda:0', grad_fn=<DivBackward0>) tensor(123123656, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6216, device='cuda:0', grad_fn=<DivBackward0>) tensor(123355114, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6211, device='cuda:0', grad_fn=<DivBackward0>) tensor(123601889, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6206, device='cuda:0', grad_fn=<DivBackward0>) tensor(123843153, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6201, device='cuda:0', grad_fn=<DivBackward0>) tensor(124082618, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6194, device='cuda:0', grad_fn=<DivBackward0>) tensor(124316694, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6189, device='cuda:0', grad_fn=<DivBackward0>) tensor(124562789, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6184, device='cuda:0', grad_fn=<DivBackward0>) tensor(124806245, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6177, device='cuda:0', grad_fn=<DivBackward0>) tensor(125047341, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6171, device='cuda:0', grad_fn=<DivBackward0>) tensor(125275519, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6167, device='cuda:0', grad_fn=<DivBackward0>) tensor(125532791, device='cuda:0')\n",
      "Starting epoch:  26\n",
      "0 0.0 2 tensor(4.6161, device='cuda:0', grad_fn=<DivBackward0>) tensor(125768815, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6155, device='cuda:0', grad_fn=<DivBackward0>) tensor(126009796, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6148, device='cuda:0', grad_fn=<DivBackward0>) tensor(126253084, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6143, device='cuda:0', grad_fn=<DivBackward0>) tensor(126513526, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6138, device='cuda:0', grad_fn=<DivBackward0>) tensor(126766040, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6132, device='cuda:0', grad_fn=<DivBackward0>) tensor(126999825, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6126, device='cuda:0', grad_fn=<DivBackward0>) tensor(127243282, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6120, device='cuda:0', grad_fn=<DivBackward0>) tensor(127477973, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6113, device='cuda:0', grad_fn=<DivBackward0>) tensor(127707198, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6108, device='cuda:0', grad_fn=<DivBackward0>) tensor(127950187, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.6102, device='cuda:0', grad_fn=<DivBackward0>) tensor(128182988, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.6098, device='cuda:0', grad_fn=<DivBackward0>) tensor(128435607, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.6093, device='cuda:0', grad_fn=<DivBackward0>) tensor(128678188, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.6087, device='cuda:0', grad_fn=<DivBackward0>) tensor(128929966, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.6081, device='cuda:0', grad_fn=<DivBackward0>) tensor(129161235, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.6075, device='cuda:0', grad_fn=<DivBackward0>) tensor(129400471, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.6070, device='cuda:0', grad_fn=<DivBackward0>) tensor(129657417, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.6067, device='cuda:0', grad_fn=<DivBackward0>) tensor(129902978, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.6061, device='cuda:0', grad_fn=<DivBackward0>) tensor(130148335, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.6057, device='cuda:0', grad_fn=<DivBackward0>) tensor(130393812, device='cuda:0')\n",
      "Starting epoch:  27\n",
      "0 0.0 2 tensor(4.6052, device='cuda:0', grad_fn=<DivBackward0>) tensor(130644365, device='cuda:0')\n",
      "200 0.05 2 tensor(4.6047, device='cuda:0', grad_fn=<DivBackward0>) tensor(130884838, device='cuda:0')\n",
      "400 0.1 2 tensor(4.6042, device='cuda:0', grad_fn=<DivBackward0>) tensor(131132347, device='cuda:0')\n",
      "600 0.15 2 tensor(4.6036, device='cuda:0', grad_fn=<DivBackward0>) tensor(131374664, device='cuda:0')\n",
      "800 0.2 2 tensor(4.6030, device='cuda:0', grad_fn=<DivBackward0>) tensor(131619431, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.6023, device='cuda:0', grad_fn=<DivBackward0>) tensor(131854895, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.6018, device='cuda:0', grad_fn=<DivBackward0>) tensor(132113623, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.6012, device='cuda:0', grad_fn=<DivBackward0>) tensor(132358585, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.6006, device='cuda:0', grad_fn=<DivBackward0>) tensor(132614694, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.6000, device='cuda:0', grad_fn=<DivBackward0>) tensor(132849618, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.5995, device='cuda:0', grad_fn=<DivBackward0>) tensor(133097376, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.5991, device='cuda:0', grad_fn=<DivBackward0>) tensor(133346398, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.5985, device='cuda:0', grad_fn=<DivBackward0>) tensor(133578151, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.5979, device='cuda:0', grad_fn=<DivBackward0>) tensor(133814608, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.5973, device='cuda:0', grad_fn=<DivBackward0>) tensor(134061131, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.5967, device='cuda:0', grad_fn=<DivBackward0>) tensor(134305599, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.5961, device='cuda:0', grad_fn=<DivBackward0>) tensor(134537373, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.5955, device='cuda:0', grad_fn=<DivBackward0>) tensor(134772952, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.5951, device='cuda:0', grad_fn=<DivBackward0>) tensor(135020102, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.5946, device='cuda:0', grad_fn=<DivBackward0>) tensor(135261081, device='cuda:0')\n",
      "Starting epoch:  28\n",
      "0 0.0 2 tensor(4.5940, device='cuda:0', grad_fn=<DivBackward0>) tensor(135507190, device='cuda:0')\n",
      "200 0.05 2 tensor(4.5934, device='cuda:0', grad_fn=<DivBackward0>) tensor(135752064, device='cuda:0')\n",
      "400 0.1 2 tensor(4.5926, device='cuda:0', grad_fn=<DivBackward0>) tensor(135990881, device='cuda:0')\n",
      "600 0.15 2 tensor(4.5921, device='cuda:0', grad_fn=<DivBackward0>) tensor(136225720, device='cuda:0')\n",
      "800 0.2 2 tensor(4.5916, device='cuda:0', grad_fn=<DivBackward0>) tensor(136459372, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.5910, device='cuda:0', grad_fn=<DivBackward0>) tensor(136696912, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.5906, device='cuda:0', grad_fn=<DivBackward0>) tensor(136939074, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.5902, device='cuda:0', grad_fn=<DivBackward0>) tensor(137189096, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.5897, device='cuda:0', grad_fn=<DivBackward0>) tensor(137425361, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.5891, device='cuda:0', grad_fn=<DivBackward0>) tensor(137675816, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.5885, device='cuda:0', grad_fn=<DivBackward0>) tensor(137910181, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.5878, device='cuda:0', grad_fn=<DivBackward0>) tensor(138149965, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.5872, device='cuda:0', grad_fn=<DivBackward0>) tensor(138384811, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.5868, device='cuda:0', grad_fn=<DivBackward0>) tensor(138635362, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2fe85915b365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimpleGPT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m# Loss reaches 6.3 after 3 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2fe85915b365>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset, optimizer, epochs, model, tokenizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2614\u001b[0m                 )\n\u001b[1;32m   2615\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2617\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2805\u001b[0m         )\n\u001b[1;32m   2806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2808\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_trie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;31m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Main loop, Giving this algorithm O(n) complexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_char\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# Prevents the lookahead for matching twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;31m# like extra_id_100 and id_100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "import random\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batch_size = 2\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            for i, e in enumerate(batch):\n",
    "                offset = random.randint(0, max(len(e) - 2048, 0))\n",
    "                batch[i] = e[offset:]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "epochs = 30\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import random\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "print(train_model(dataset[:2000*4], optimizer, epochs, simpleGPT2, tokenizer))\n",
    "# Loss reaches 6.3 after 3 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e57a0e7-f47b-4f17-a8be-dde9a12f4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b49424ce377b>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she?\n",
      "\n",
      " and great days. A. H\n",
      "\n",
      " A of devastation as well as it for the last season (Coloming you want her more) because I want to visit this stage that has been on the US (and this is (in of) actually) but it is definitely is about that if they ever threatened to win or so that's the real (and if they shouldn't want to win a fair than any person knows about 4 years), we understand what happened with a surprise as well\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she? And how did they decide she was a vampire?\"\n",
      "\n",
      "\n",
      "Kolto was an easy way to end this story through the back story or through an alternate canon. You'll probably have to check out some of the canon first, but try to keep away from the main character, especially his name. It certainly could've been my favorite character to end the story, but Kolto was actually just getting started.\n",
      "\n",
      "\n",
      "Also, most of the previous issues featured recurring heroes, like Vampiri\n",
      "0 10 0 0\n",
      "1 10 tensor(5.6152, device='cuda:0') tensor(6150, device='cuda:0')\n",
      "2 10 tensor(5.6645, device='cuda:0') tensor(12072, device='cuda:0')\n",
      "3 10 tensor(5.7183, device='cuda:0') tensor(17701, device='cuda:0')\n",
      "4 10 tensor(5.7723, device='cuda:0') tensor(22832, device='cuda:0')\n",
      "5 10 tensor(5.8115, device='cuda:0') tensor(27643, device='cuda:0')\n",
      "6 10 tensor(5.8480, device='cuda:0') tensor(32385, device='cuda:0')\n",
      "7 10 tensor(5.8538, device='cuda:0') tensor(37210, device='cuda:0')\n",
      "8 10 tensor(5.8669, device='cuda:0') tensor(42070, device='cuda:0')\n",
      "9 10 tensor(5.8827, device='cuda:0') tensor(47111, device='cuda:0')\n",
      "10 10 tensor(5.8891, device='cuda:0') tensor(52337, device='cuda:0')\n",
      "11 10 tensor(5.8917, device='cuda:0') tensor(58158, device='cuda:0')\n",
      "12 10 tensor(5.8973, device='cuda:0') tensor(64330, device='cuda:0')\n",
      "13 10 tensor(5.9054, device='cuda:0') tensor(71107, device='cuda:0')\n",
      "14 10 tensor(5.9140, device='cuda:0') tensor(77771, device='cuda:0')\n",
      "15 10 tensor(5.9236, device='cuda:0') tensor(84310, device='cuda:0')\n",
      "16 10 tensor(5.9392, device='cuda:0') tensor(90440, device='cuda:0')\n",
      "17 10 tensor(5.9433, device='cuda:0') tensor(96570, device='cuda:0')\n",
      "18 10 tensor(5.9472, device='cuda:0') tensor(103026, device='cuda:0')\n",
      "19 10 tensor(5.9480, device='cuda:0') tensor(109885, device='cuda:0')\n",
      "20 10 tensor(5.9450, device='cuda:0') tensor(117341, device='cuda:0')\n",
      "21 10 tensor(5.9446, device='cuda:0') tensor(124365, device='cuda:0')\n",
      "22 10 tensor(5.9397, device='cuda:0') tensor(131298, device='cuda:0')\n",
      "23 10 tensor(5.9298, device='cuda:0') tensor(137804, device='cuda:0')\n",
      "24 10 tensor(5.9187, device='cuda:0') tensor(144491, device='cuda:0')\n",
      "25 10 tensor(5.9092, device='cuda:0') tensor(151286, device='cuda:0')\n",
      "26 10 tensor(5.8983, device='cuda:0') tensor(158129, device='cuda:0')\n",
      "27 10 tensor(5.8941, device='cuda:0') tensor(164972, device='cuda:0')\n",
      "28 10 tensor(5.8932, device='cuda:0') tensor(171515, device='cuda:0')\n",
      "29 10 tensor(5.8944, device='cuda:0') tensor(178147, device='cuda:0')\n",
      "30 10 tensor(5.8982, device='cuda:0') tensor(184434, device='cuda:0')\n",
      "31 10 tensor(5.9027, device='cuda:0') tensor(190970, device='cuda:0')\n",
      "32 10 tensor(5.9062, device='cuda:0') tensor(197585, device='cuda:0')\n",
      "33 10 tensor(5.9103, device='cuda:0') tensor(204475, device='cuda:0')\n",
      "34 10 tensor(5.9146, device='cuda:0') tensor(211492, device='cuda:0')\n",
      "35 10 tensor(5.9197, device='cuda:0') tensor(218281, device='cuda:0')\n",
      "36 10 tensor(5.9275, device='cuda:0') tensor(225245, device='cuda:0')\n",
      "37 10 tensor(5.9329, device='cuda:0') tensor(231614, device='cuda:0')\n",
      "38 10 tensor(5.9354, device='cuda:0') tensor(238102, device='cuda:0')\n",
      "39 10 tensor(5.9382, device='cuda:0') tensor(243865, device='cuda:0')\n",
      "40 10 tensor(5.9392, device='cuda:0') tensor(249973, device='cuda:0')\n",
      "41 10 tensor(5.9400, device='cuda:0') tensor(255473, device='cuda:0')\n",
      "42 10 tensor(5.9450, device='cuda:0') tensor(261392, device='cuda:0')\n",
      "43 10 tensor(5.9495, device='cuda:0') tensor(267042, device='cuda:0')\n",
      "44 10 tensor(5.9548, device='cuda:0') tensor(273097, device='cuda:0')\n",
      "45 10 tensor(5.9595, device='cuda:0') tensor(279262, device='cuda:0')\n",
      "46 10 tensor(5.9620, device='cuda:0') tensor(285196, device='cuda:0')\n",
      "47 10 tensor(5.9649, device='cuda:0') tensor(291212, device='cuda:0')\n",
      "48 10 tensor(5.9678, device='cuda:0') tensor(297861, device='cuda:0')\n",
      "49 10 tensor(5.9707, device='cuda:0') tensor(304502, device='cuda:0')\n",
      "50 10 tensor(5.9742, device='cuda:0') tensor(310906, device='cuda:0')\n",
      "51 10 tensor(5.9795, device='cuda:0') tensor(317998, device='cuda:0')\n",
      "52 10 tensor(5.9854, device='cuda:0') tensor(325090, device='cuda:0')\n",
      "53 10 tensor(5.9912, device='cuda:0') tensor(331817, device='cuda:0')\n",
      "54 10 tensor(5.9941, device='cuda:0') tensor(338544, device='cuda:0')\n",
      "55 10 tensor(5.9968, device='cuda:0') tensor(345916, device='cuda:0')\n",
      "56 10 tensor(6.0005, device='cuda:0') tensor(353223, device='cuda:0')\n",
      "57 10 tensor(6.0032, device='cuda:0') tensor(360641, device='cuda:0')\n",
      "58 10 tensor(6.0074, device='cuda:0') tensor(367353, device='cuda:0')\n",
      "59 10 tensor(6.0100, device='cuda:0') tensor(374671, device='cuda:0')\n",
      "60 10 tensor(6.0121, device='cuda:0') tensor(381466, device='cuda:0')\n",
      "61 10 tensor(6.0128, device='cuda:0') tensor(388364, device='cuda:0')\n",
      "62 10 tensor(6.0109, device='cuda:0') tensor(394457, device='cuda:0')\n",
      "63 10 tensor(6.0104, device='cuda:0') tensor(401190, device='cuda:0')\n",
      "64 10 tensor(6.0106, device='cuda:0') tensor(407923, device='cuda:0')\n",
      "65 10 tensor(6.0103, device='cuda:0') tensor(414485, device='cuda:0')\n",
      "66 10 tensor(6.0079, device='cuda:0') tensor(421598, device='cuda:0')\n",
      "67 10 tensor(6.0051, device='cuda:0') tensor(428664, device='cuda:0')\n",
      "68 10 tensor(6.0022, device='cuda:0') tensor(435707, device='cuda:0')\n",
      "69 10 tensor(6.0004, device='cuda:0') tensor(442328, device='cuda:0')\n",
      "70 10 tensor(5.9978, device='cuda:0') tensor(449266, device='cuda:0')\n",
      "71 10 tensor(5.9964, device='cuda:0') tensor(455799, device='cuda:0')\n",
      "72 10 tensor(5.9954, device='cuda:0') tensor(463137, device='cuda:0')\n",
      "73 10 tensor(5.9936, device='cuda:0') tensor(470621, device='cuda:0')\n",
      "74 10 tensor(5.9924, device='cuda:0') tensor(477479, device='cuda:0')\n",
      "75 10 tensor(5.9921, device='cuda:0') tensor(483965, device='cuda:0')\n",
      "76 10 tensor(5.9929, device='cuda:0') tensor(489770, device='cuda:0')\n",
      "77 10 tensor(5.9931, device='cuda:0') tensor(496024, device='cuda:0')\n",
      "78 10 tensor(5.9934, device='cuda:0') tensor(502638, device='cuda:0')\n",
      "79 10 tensor(5.9943, device='cuda:0') tensor(508989, device='cuda:0')\n",
      "80 10 tensor(5.9976, device='cuda:0') tensor(515682, device='cuda:0')\n",
      "81 10 tensor(6.0002, device='cuda:0') tensor(522024, device='cuda:0')\n",
      "82 10 tensor(6.0019, device='cuda:0') tensor(528366, device='cuda:0')\n",
      "83 10 tensor(6.0041, device='cuda:0') tensor(534245, device='cuda:0')\n",
      "84 10 tensor(6.0048, device='cuda:0') tensor(540438, device='cuda:0')\n",
      "85 10 tensor(6.0068, device='cuda:0') tensor(547174, device='cuda:0')\n",
      "86 10 tensor(6.0083, device='cuda:0') tensor(554515, device='cuda:0')\n",
      "87 10 tensor(6.0124, device='cuda:0') tensor(561856, device='cuda:0')\n",
      "88 10 tensor(6.0164, device='cuda:0') tensor(569566, device='cuda:0')\n",
      "89 10 tensor(6.0206, device='cuda:0') tensor(577440, device='cuda:0')\n",
      "90 10 tensor(6.0233, device='cuda:0') tensor(584629, device='cuda:0')\n",
      "91 10 tensor(6.0259, device='cuda:0') tensor(591912, device='cuda:0')\n",
      "92 10 tensor(6.0296, device='cuda:0') tensor(599195, device='cuda:0')\n",
      "93 10 tensor(6.0321, device='cuda:0') tensor(606449, device='cuda:0')\n",
      "94 10 tensor(6.0360, device='cuda:0') tensor(614015, device='cuda:0')\n",
      "95 10 tensor(6.0379, device='cuda:0') tensor(621581, device='cuda:0')\n",
      "96 10 tensor(6.0402, device='cuda:0') tensor(628898, device='cuda:0')\n",
      "97 10 tensor(6.0471, device='cuda:0') tensor(636140, device='cuda:0')\n",
      "98 10 tensor(6.0520, device='cuda:0') tensor(642804, device='cuda:0')\n",
      "99 10 tensor(6.0568, device='cuda:0') tensor(650116, device='cuda:0')\n",
      "(tensor(6.0610, device='cuda:0'), tensor(657621, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))\n",
    "print(compute_val_dataset_loss(dataset, simpleGPT2, tokenizer, 0.1))\n",
    "\n",
    "# Generations seem to be getting better, or maybe that's just my imagination.\n",
    "# After 105 epochs, loss on val set is around 6.05, and training loss has converged to ~4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e5090d7-b32c-4fad-8f31-55bbe60558b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(simpleGPT2, 'simpleGPT_105_epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a369956-595b-4feb-bffc-2e3b90f994fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
