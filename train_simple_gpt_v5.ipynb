{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eed89d3-3348-46d6-a786-90ebb4017feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 regex-2023.3.23 tokenizers-0.13.2 transformers-4.27.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from datasets) (1.23.4)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (from datasets) (1.5.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./.local/lib/python3.8/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, urllib3, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, responses, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.3.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 urllib3-1.26.15 xxhash-3.2.0 yarl-1.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43aacda-f79c-4bdd-97ce-030ae0499c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fa9237d01642a484ece3937a5d2cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d29c63387244b7fae6a19381ec0d960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6388d786004ddc8d4163083a637696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fca2b7033e54dc09f8ca8a0aec5d7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ced53e867f4dcaa6cba27da627cab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa6fd8-d3d0-4dbc-be81-e531a5441970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b55d8dee647ef8968fdb7ec79fde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bc6dc717174d3b950d97d964b2ef19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72e291062e8462c9d6ee115831832b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset openwebtext-10k/plain_text (download: 14.04 MiB, generated: 47.37 MiB, post-processed: Unknown size, total: 61.41 MiB) to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c6f66695b42609e42bb0aeba80f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/download/download_manager.py:527: FutureWarning: 'num_proc' was deprecated in version 2.6.2 and will be removed in 3.0.0. Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6548c1c22004b669e474e1ca403c667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openwebtext-10k downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d0d03a236940cfbcdd9a3123907060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700c4c5a-f2b9-4e1c-9d7d-5fa0965a73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bac5cbb-245c-473b-a5c0-25394f1568b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396d59ee-132c-4bd0-9a13-0c5776d0cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33624b5b-64ba-45d3-97b7-2875ef37482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072ba405-152a-44d1-b854-22cb180f8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
      "torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "simpleGPT2 = t.load('simpleGPT_12_epochs.pkl')\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3518804f-6a4e-4c3e-b639-d06846eb5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120560209\n"
     ]
    }
   ],
   "source": [
    "# How many parameters?\n",
    "print(sum((p.numel() if p.requires_grad else 0 for p in simpleGPT2.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22e1960-82f0-4ad8-8172-7c09bf0f9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(21, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoded_input_alt = tokenizer(dataset[0][:100], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input_alt['attention_mask'].shape, encoded_input_alt['attention_mask'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b894e93f-04dc-42dc-9fa9-f54c88103b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "  return logits.argmax()\n",
    "\n",
    "def test_model(model, text = \"Replace me by any text you'd like.\", steps = 100, sampling = greedy_sampling, is_hf = False):\n",
    "    eos_token = \"<|endoftext|>\"\n",
    "    prompt = text\n",
    "    print(\"Starting prompt: \" + prompt)\n",
    "\n",
    "    for i in range(steps):\n",
    "        encoded_input = tokenizer([prompt], return_tensors=\"pt\", padding='max_length').to(device)\n",
    "        last_input_idx = encoded_input['attention_mask'][0].sum() - 1\n",
    "        if is_hf:\n",
    "            logits = model(**encoded_input).logits[0, last_input_idx]\n",
    "        else:\n",
    "            logits = model(**encoded_input)[0, last_input_idx]\n",
    "        next_token = sampling(logits)\n",
    "        next_string = tokenizer.decode(next_token)\n",
    "        if next_string == eos_token:\n",
    "            break\n",
    "        prompt = prompt + next_string\n",
    "    print(\"Current generation: \" + prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5d6e6d-7c3b-4849-9571-bbf505043c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-cd70a51e302d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n",
      "<ipython-input-15-cd70a51e302d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she? In a child over many cases, and a case-term in the right: To handle the most chance a century, though in the State; or even more than the National), who would lead to sell the public for your dog.\n",
      "With the car a very key as the final city and more than any in least the time and the latest to do-year-back, you might be as the truth the book! The city who were to be, and the right a \"The day with\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she?\n",
      "\n",
      "\n",
      "Yes.\n",
      "\n",
      "\n",
      "You were born of a girl, a poor woman, in a village, before the flood on the Eastern shore. Was that her name—\n",
      "\n",
      "\n",
      "The Lady From Sandil, Lady Arianne, the Lady of The Forest; and why did she call you, and where did she come from?\n",
      "\n",
      "\n",
      "We have seen that she was born of a woman and had been raised by three-year-old Cuthydna. Your father's name\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(k):\n",
    "\n",
    "\n",
    "      def top_sampling(logits):\n",
    "          probs = t.nn.functional.softmax(logits)\n",
    "          values, indices = t.topk(probs, k)\n",
    "          index = values.multinomial(num_samples = 1, replacement = True)\n",
    "          return indices[index]\n",
    "      \n",
    "      return top_sampling\n",
    "\n",
    "# Our model generates English, but not really coherent generations.\n",
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fde2a27-c08a-4a3c-bfaa-deadcc89a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 0 0\n",
      "1 10 tensor(6.1029, device='cuda:0') tensor(6508, device='cuda:0')\n",
      "2 10 tensor(6.1376, device='cuda:0') tensor(12338, device='cuda:0')\n",
      "3 10 tensor(6.1385, device='cuda:0') tensor(18466, device='cuda:0')\n",
      "4 10 tensor(6.1180, device='cuda:0') tensor(24853, device='cuda:0')\n",
      "5 10 tensor(6.1179, device='cuda:0') tensor(30959, device='cuda:0')\n",
      "6 10 tensor(6.1087, device='cuda:0') tensor(37244, device='cuda:0')\n",
      "7 10 tensor(6.0942, device='cuda:0') tensor(43529, device='cuda:0')\n",
      "8 10 tensor(6.0782, device='cuda:0') tensor(50599, device='cuda:0')\n",
      "9 10 tensor(6.0713, device='cuda:0') tensor(57171, device='cuda:0')\n",
      "(tensor(6.0648, device='cuda:0'), tensor(63899, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batch_size = 10\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples\n",
    "\n",
    "# Compute loss of the pre-trained model on the truncated dataset\n",
    "print(compute_dataset_loss(dataset[:100], simpleGPT2, tokenizer))\n",
    "\n",
    "# Initial loss is ~6, better than 11, but still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de997d79-36e8-462f-84fb-742b2cd7c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_dataset_loss(dataset, model, tokenizer, val_frac = 0.2):\n",
    "    n = len(dataset)\n",
    "    val_size = int(n * val_frac)\n",
    "    return compute_dataset_loss(dataset[-val_size:], model, tokenizer)\n",
    "  \n",
    "# Compute validation loss\n",
    "# print(compute_val_dataset_loss(dataset, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402e6ee2-d473-4426-b171-16598bea8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "50 0.05 2 tensor(6.1689, device='cuda:0', grad_fn=<DivBackward0>) tensor(62993, device='cuda:0')\n",
      "100 0.1 2 tensor(6.1812, device='cuda:0', grad_fn=<DivBackward0>) tensor(123820, device='cuda:0')\n",
      "150 0.15 2 tensor(6.2022, device='cuda:0', grad_fn=<DivBackward0>) tensor(181542, device='cuda:0')\n",
      "200 0.2 2 tensor(6.2032, device='cuda:0', grad_fn=<DivBackward0>) tensor(240214, device='cuda:0')\n",
      "250 0.25 2 tensor(6.2045, device='cuda:0', grad_fn=<DivBackward0>) tensor(295027, device='cuda:0')\n",
      "300 0.3 2 tensor(6.2157, device='cuda:0', grad_fn=<DivBackward0>) tensor(350924, device='cuda:0')\n",
      "350 0.35 2 tensor(6.2149, device='cuda:0', grad_fn=<DivBackward0>) tensor(410743, device='cuda:0')\n",
      "400 0.4 2 tensor(6.2020, device='cuda:0', grad_fn=<DivBackward0>) tensor(471080, device='cuda:0')\n",
      "450 0.45 2 tensor(6.2005, device='cuda:0', grad_fn=<DivBackward0>) tensor(534108, device='cuda:0')\n",
      "500 0.5 2 tensor(6.2032, device='cuda:0', grad_fn=<DivBackward0>) tensor(594198, device='cuda:0')\n",
      "550 0.55 2 tensor(6.1964, device='cuda:0', grad_fn=<DivBackward0>) tensor(654882, device='cuda:0')\n",
      "600 0.6 2 tensor(6.1963, device='cuda:0', grad_fn=<DivBackward0>) tensor(716516, device='cuda:0')\n",
      "650 0.65 2 tensor(6.1989, device='cuda:0', grad_fn=<DivBackward0>) tensor(776562, device='cuda:0')\n",
      "700 0.7 2 tensor(6.2019, device='cuda:0', grad_fn=<DivBackward0>) tensor(837715, device='cuda:0')\n",
      "750 0.75 2 tensor(6.2078, device='cuda:0', grad_fn=<DivBackward0>) tensor(892168, device='cuda:0')\n",
      "800 0.8 2 tensor(6.2043, device='cuda:0', grad_fn=<DivBackward0>) tensor(950400, device='cuda:0')\n",
      "850 0.85 2 tensor(6.2009, device='cuda:0', grad_fn=<DivBackward0>) tensor(1010974, device='cuda:0')\n",
      "900 0.9 2 tensor(6.2008, device='cuda:0', grad_fn=<DivBackward0>) tensor(1066008, device='cuda:0')\n",
      "950 0.95 2 tensor(6.2124, device='cuda:0', grad_fn=<DivBackward0>) tensor(1125853, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(6.2101, device='cuda:0', grad_fn=<DivBackward0>) tensor(1181875, device='cuda:0')\n",
      "50 0.05 2 tensor(6.2022, device='cuda:0', grad_fn=<DivBackward0>) tensor(1240109, device='cuda:0')\n",
      "100 0.1 2 tensor(6.1974, device='cuda:0', grad_fn=<DivBackward0>) tensor(1296712, device='cuda:0')\n",
      "150 0.15 2 tensor(6.1944, device='cuda:0', grad_fn=<DivBackward0>) tensor(1359255, device='cuda:0')\n",
      "200 0.2 2 tensor(6.1832, device='cuda:0', grad_fn=<DivBackward0>) tensor(1417800, device='cuda:0')\n",
      "250 0.25 2 tensor(6.1835, device='cuda:0', grad_fn=<DivBackward0>) tensor(1478910, device='cuda:0')\n",
      "300 0.3 2 tensor(6.1766, device='cuda:0', grad_fn=<DivBackward0>) tensor(1536410, device='cuda:0')\n",
      "350 0.35 2 tensor(6.1676, device='cuda:0', grad_fn=<DivBackward0>) tensor(1597071, device='cuda:0')\n",
      "400 0.4 2 tensor(6.1634, device='cuda:0', grad_fn=<DivBackward0>) tensor(1658403, device='cuda:0')\n",
      "450 0.45 2 tensor(6.1649, device='cuda:0', grad_fn=<DivBackward0>) tensor(1711856, device='cuda:0')\n",
      "500 0.5 2 tensor(6.1690, device='cuda:0', grad_fn=<DivBackward0>) tensor(1763027, device='cuda:0')\n",
      "550 0.55 2 tensor(6.1667, device='cuda:0', grad_fn=<DivBackward0>) tensor(1820218, device='cuda:0')\n",
      "600 0.6 2 tensor(6.1680, device='cuda:0', grad_fn=<DivBackward0>) tensor(1880126, device='cuda:0')\n",
      "650 0.65 2 tensor(6.1632, device='cuda:0', grad_fn=<DivBackward0>) tensor(1946306, device='cuda:0')\n",
      "700 0.7 2 tensor(6.1633, device='cuda:0', grad_fn=<DivBackward0>) tensor(2008646, device='cuda:0')\n",
      "750 0.75 2 tensor(6.1631, device='cuda:0', grad_fn=<DivBackward0>) tensor(2071316, device='cuda:0')\n",
      "800 0.8 2 tensor(6.1591, device='cuda:0', grad_fn=<DivBackward0>) tensor(2134397, device='cuda:0')\n",
      "850 0.85 2 tensor(6.1568, device='cuda:0', grad_fn=<DivBackward0>) tensor(2189929, device='cuda:0')\n",
      "900 0.9 2 tensor(6.1541, device='cuda:0', grad_fn=<DivBackward0>) tensor(2245367, device='cuda:0')\n",
      "950 0.95 2 tensor(6.1578, device='cuda:0', grad_fn=<DivBackward0>) tensor(2309321, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(6.1546, device='cuda:0', grad_fn=<DivBackward0>) tensor(2368648, device='cuda:0')\n",
      "50 0.05 2 tensor(6.1520, device='cuda:0', grad_fn=<DivBackward0>) tensor(2431508, device='cuda:0')\n",
      "100 0.1 2 tensor(6.1434, device='cuda:0', grad_fn=<DivBackward0>) tensor(2494265, device='cuda:0')\n",
      "150 0.15 2 tensor(6.1372, device='cuda:0', grad_fn=<DivBackward0>) tensor(2559501, device='cuda:0')\n",
      "200 0.2 2 tensor(6.1367, device='cuda:0', grad_fn=<DivBackward0>) tensor(2618323, device='cuda:0')\n",
      "250 0.25 2 tensor(6.1345, device='cuda:0', grad_fn=<DivBackward0>) tensor(2681669, device='cuda:0')\n",
      "300 0.3 2 tensor(6.1338, device='cuda:0', grad_fn=<DivBackward0>) tensor(2740189, device='cuda:0')\n",
      "350 0.35 2 tensor(6.1290, device='cuda:0', grad_fn=<DivBackward0>) tensor(2795656, device='cuda:0')\n",
      "400 0.4 2 tensor(6.1284, device='cuda:0', grad_fn=<DivBackward0>) tensor(2855768, device='cuda:0')\n",
      "450 0.45 2 tensor(6.1242, device='cuda:0', grad_fn=<DivBackward0>) tensor(2918979, device='cuda:0')\n",
      "500 0.5 2 tensor(6.1233, device='cuda:0', grad_fn=<DivBackward0>) tensor(2980368, device='cuda:0')\n",
      "550 0.55 2 tensor(6.1185, device='cuda:0', grad_fn=<DivBackward0>) tensor(3034547, device='cuda:0')\n",
      "600 0.6 2 tensor(6.1143, device='cuda:0', grad_fn=<DivBackward0>) tensor(3089408, device='cuda:0')\n",
      "650 0.65 2 tensor(6.1103, device='cuda:0', grad_fn=<DivBackward0>) tensor(3146312, device='cuda:0')\n",
      "700 0.7 2 tensor(6.1078, device='cuda:0', grad_fn=<DivBackward0>) tensor(3203151, device='cuda:0')\n",
      "750 0.75 2 tensor(6.1057, device='cuda:0', grad_fn=<DivBackward0>) tensor(3266157, device='cuda:0')\n",
      "800 0.8 2 tensor(6.1023, device='cuda:0', grad_fn=<DivBackward0>) tensor(3333474, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0993, device='cuda:0', grad_fn=<DivBackward0>) tensor(3387447, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0971, device='cuda:0', grad_fn=<DivBackward0>) tensor(3449083, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0944, device='cuda:0', grad_fn=<DivBackward0>) tensor(3508294, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(6.0919, device='cuda:0', grad_fn=<DivBackward0>) tensor(3572692, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0866, device='cuda:0', grad_fn=<DivBackward0>) tensor(3636321, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0823, device='cuda:0', grad_fn=<DivBackward0>) tensor(3703406, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0764, device='cuda:0', grad_fn=<DivBackward0>) tensor(3768042, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0743, device='cuda:0', grad_fn=<DivBackward0>) tensor(3822368, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0729, device='cuda:0', grad_fn=<DivBackward0>) tensor(3882059, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0688, device='cuda:0', grad_fn=<DivBackward0>) tensor(3933741, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0657, device='cuda:0', grad_fn=<DivBackward0>) tensor(3991773, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0645, device='cuda:0', grad_fn=<DivBackward0>) tensor(4046662, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0631, device='cuda:0', grad_fn=<DivBackward0>) tensor(4107765, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0594, device='cuda:0', grad_fn=<DivBackward0>) tensor(4164282, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0562, device='cuda:0', grad_fn=<DivBackward0>) tensor(4219785, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0514, device='cuda:0', grad_fn=<DivBackward0>) tensor(4279727, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0481, device='cuda:0', grad_fn=<DivBackward0>) tensor(4338419, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0455, device='cuda:0', grad_fn=<DivBackward0>) tensor(4403124, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0424, device='cuda:0', grad_fn=<DivBackward0>) tensor(4466110, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0412, device='cuda:0', grad_fn=<DivBackward0>) tensor(4521544, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0382, device='cuda:0', grad_fn=<DivBackward0>) tensor(4590767, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0360, device='cuda:0', grad_fn=<DivBackward0>) tensor(4647706, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0331, device='cuda:0', grad_fn=<DivBackward0>) tensor(4714405, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(6.0303, device='cuda:0', grad_fn=<DivBackward0>) tensor(4777218, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0269, device='cuda:0', grad_fn=<DivBackward0>) tensor(4830451, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0231, device='cuda:0', grad_fn=<DivBackward0>) tensor(4893076, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0208, device='cuda:0', grad_fn=<DivBackward0>) tensor(4950418, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0163, device='cuda:0', grad_fn=<DivBackward0>) tensor(5011494, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0107, device='cuda:0', grad_fn=<DivBackward0>) tensor(5072641, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0063, device='cuda:0', grad_fn=<DivBackward0>) tensor(5131294, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0026, device='cuda:0', grad_fn=<DivBackward0>) tensor(5193381, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0001, device='cuda:0', grad_fn=<DivBackward0>) tensor(5253843, device='cuda:0')\n",
      "450 0.45 2 tensor(5.9979, device='cuda:0', grad_fn=<DivBackward0>) tensor(5313559, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9955, device='cuda:0', grad_fn=<DivBackward0>) tensor(5372043, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9935, device='cuda:0', grad_fn=<DivBackward0>) tensor(5435119, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9905, device='cuda:0', grad_fn=<DivBackward0>) tensor(5489842, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9877, device='cuda:0', grad_fn=<DivBackward0>) tensor(5546235, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9850, device='cuda:0', grad_fn=<DivBackward0>) tensor(5607286, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9810, device='cuda:0', grad_fn=<DivBackward0>) tensor(5664410, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9809, device='cuda:0', grad_fn=<DivBackward0>) tensor(5727917, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9787, device='cuda:0', grad_fn=<DivBackward0>) tensor(5788064, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9750, device='cuda:0', grad_fn=<DivBackward0>) tensor(5838409, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9731, device='cuda:0', grad_fn=<DivBackward0>) tensor(5903673, device='cuda:0')\n",
      "(tensor(5.9700, device='cuda:0', grad_fn=<DivBackward0>), tensor(5960702, device='cuda:0'))\n",
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "50 0.05 2 tensor(6.0452, device='cuda:0', grad_fn=<DivBackward0>) tensor(54083, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0683, device='cuda:0', grad_fn=<DivBackward0>) tensor(112628, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0649, device='cuda:0', grad_fn=<DivBackward0>) tensor(173157, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0584, device='cuda:0', grad_fn=<DivBackward0>) tensor(233307, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0787, device='cuda:0', grad_fn=<DivBackward0>) tensor(295128, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0732, device='cuda:0', grad_fn=<DivBackward0>) tensor(358558, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0885, device='cuda:0', grad_fn=<DivBackward0>) tensor(427147, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0785, device='cuda:0', grad_fn=<DivBackward0>) tensor(487872, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0824, device='cuda:0', grad_fn=<DivBackward0>) tensor(554981, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0761, device='cuda:0', grad_fn=<DivBackward0>) tensor(620480, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0840, device='cuda:0', grad_fn=<DivBackward0>) tensor(679808, device='cuda:0')\n",
      "600 0.6 2 tensor(6.1012, device='cuda:0', grad_fn=<DivBackward0>) tensor(737390, device='cuda:0')\n",
      "650 0.65 2 tensor(6.1004, device='cuda:0', grad_fn=<DivBackward0>) tensor(798560, device='cuda:0')\n",
      "700 0.7 2 tensor(6.1040, device='cuda:0', grad_fn=<DivBackward0>) tensor(860010, device='cuda:0')\n",
      "750 0.75 2 tensor(6.1065, device='cuda:0', grad_fn=<DivBackward0>) tensor(921724, device='cuda:0')\n",
      "800 0.8 2 tensor(6.1024, device='cuda:0', grad_fn=<DivBackward0>) tensor(976035, device='cuda:0')\n",
      "850 0.85 2 tensor(6.1017, device='cuda:0', grad_fn=<DivBackward0>) tensor(1036581, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0999, device='cuda:0', grad_fn=<DivBackward0>) tensor(1091228, device='cuda:0')\n",
      "950 0.95 2 tensor(6.1026, device='cuda:0', grad_fn=<DivBackward0>) tensor(1148674, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(6.0979, device='cuda:0', grad_fn=<DivBackward0>) tensor(1206282, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0901, device='cuda:0', grad_fn=<DivBackward0>) tensor(1264255, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0890, device='cuda:0', grad_fn=<DivBackward0>) tensor(1327813, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0877, device='cuda:0', grad_fn=<DivBackward0>) tensor(1389472, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0815, device='cuda:0', grad_fn=<DivBackward0>) tensor(1450945, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0791, device='cuda:0', grad_fn=<DivBackward0>) tensor(1509869, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0746, device='cuda:0', grad_fn=<DivBackward0>) tensor(1572922, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0723, device='cuda:0', grad_fn=<DivBackward0>) tensor(1628386, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0769, device='cuda:0', grad_fn=<DivBackward0>) tensor(1691052, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0770, device='cuda:0', grad_fn=<DivBackward0>) tensor(1753355, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0738, device='cuda:0', grad_fn=<DivBackward0>) tensor(1811636, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0742, device='cuda:0', grad_fn=<DivBackward0>) tensor(1876916, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0779, device='cuda:0', grad_fn=<DivBackward0>) tensor(1937218, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0781, device='cuda:0', grad_fn=<DivBackward0>) tensor(1998456, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0807, device='cuda:0', grad_fn=<DivBackward0>) tensor(2061212, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0790, device='cuda:0', grad_fn=<DivBackward0>) tensor(2122341, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0756, device='cuda:0', grad_fn=<DivBackward0>) tensor(2181814, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0749, device='cuda:0', grad_fn=<DivBackward0>) tensor(2236537, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0728, device='cuda:0', grad_fn=<DivBackward0>) tensor(2291448, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0725, device='cuda:0', grad_fn=<DivBackward0>) tensor(2354905, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(6.0695, device='cuda:0', grad_fn=<DivBackward0>) tensor(2419310, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0666, device='cuda:0', grad_fn=<DivBackward0>) tensor(2490412, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0638, device='cuda:0', grad_fn=<DivBackward0>) tensor(2555965, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0585, device='cuda:0', grad_fn=<DivBackward0>) tensor(2614598, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0531, device='cuda:0', grad_fn=<DivBackward0>) tensor(2671855, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0502, device='cuda:0', grad_fn=<DivBackward0>) tensor(2733148, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0510, device='cuda:0', grad_fn=<DivBackward0>) tensor(2797007, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0504, device='cuda:0', grad_fn=<DivBackward0>) tensor(2861790, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0464, device='cuda:0', grad_fn=<DivBackward0>) tensor(2922441, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0431, device='cuda:0', grad_fn=<DivBackward0>) tensor(2982252, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0390, device='cuda:0', grad_fn=<DivBackward0>) tensor(3040412, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0360, device='cuda:0', grad_fn=<DivBackward0>) tensor(3096387, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0358, device='cuda:0', grad_fn=<DivBackward0>) tensor(3156693, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0362, device='cuda:0', grad_fn=<DivBackward0>) tensor(3216866, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0340, device='cuda:0', grad_fn=<DivBackward0>) tensor(3281619, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0346, device='cuda:0', grad_fn=<DivBackward0>) tensor(3342705, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0338, device='cuda:0', grad_fn=<DivBackward0>) tensor(3401970, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0309, device='cuda:0', grad_fn=<DivBackward0>) tensor(3462384, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0301, device='cuda:0', grad_fn=<DivBackward0>) tensor(3529332, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0293, device='cuda:0', grad_fn=<DivBackward0>) tensor(3593686, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(6.0295, device='cuda:0', grad_fn=<DivBackward0>) tensor(3654444, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0238, device='cuda:0', grad_fn=<DivBackward0>) tensor(3716326, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0198, device='cuda:0', grad_fn=<DivBackward0>) tensor(3776747, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0165, device='cuda:0', grad_fn=<DivBackward0>) tensor(3834187, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0133, device='cuda:0', grad_fn=<DivBackward0>) tensor(3894602, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0081, device='cuda:0', grad_fn=<DivBackward0>) tensor(3956208, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0067, device='cuda:0', grad_fn=<DivBackward0>) tensor(4014720, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0035, device='cuda:0', grad_fn=<DivBackward0>) tensor(4072634, device='cuda:0')\n",
      "400 0.4 2 tensor(5.9981, device='cuda:0', grad_fn=<DivBackward0>) tensor(4128424, device='cuda:0')\n",
      "450 0.45 2 tensor(5.9938, device='cuda:0', grad_fn=<DivBackward0>) tensor(4193019, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9901, device='cuda:0', grad_fn=<DivBackward0>) tensor(4250323, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9877, device='cuda:0', grad_fn=<DivBackward0>) tensor(4301852, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9857, device='cuda:0', grad_fn=<DivBackward0>) tensor(4361547, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9829, device='cuda:0', grad_fn=<DivBackward0>) tensor(4422055, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9801, device='cuda:0', grad_fn=<DivBackward0>) tensor(4487435, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9769, device='cuda:0', grad_fn=<DivBackward0>) tensor(4548270, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9728, device='cuda:0', grad_fn=<DivBackward0>) tensor(4612539, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9697, device='cuda:0', grad_fn=<DivBackward0>) tensor(4669308, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9678, device='cuda:0', grad_fn=<DivBackward0>) tensor(4730607, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9644, device='cuda:0', grad_fn=<DivBackward0>) tensor(4787818, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(5.9615, device='cuda:0', grad_fn=<DivBackward0>) tensor(4849694, device='cuda:0')\n",
      "50 0.05 2 tensor(5.9577, device='cuda:0', grad_fn=<DivBackward0>) tensor(4905841, device='cuda:0')\n",
      "100 0.1 2 tensor(5.9539, device='cuda:0', grad_fn=<DivBackward0>) tensor(4969401, device='cuda:0')\n",
      "150 0.15 2 tensor(5.9503, device='cuda:0', grad_fn=<DivBackward0>) tensor(5029751, device='cuda:0')\n",
      "200 0.2 2 tensor(5.9453, device='cuda:0', grad_fn=<DivBackward0>) tensor(5087719, device='cuda:0')\n",
      "250 0.25 2 tensor(5.9421, device='cuda:0', grad_fn=<DivBackward0>) tensor(5147909, device='cuda:0')\n",
      "300 0.3 2 tensor(5.9380, device='cuda:0', grad_fn=<DivBackward0>) tensor(5203572, device='cuda:0')\n",
      "350 0.35 2 tensor(5.9335, device='cuda:0', grad_fn=<DivBackward0>) tensor(5259297, device='cuda:0')\n",
      "400 0.4 2 tensor(5.9303, device='cuda:0', grad_fn=<DivBackward0>) tensor(5325903, device='cuda:0')\n",
      "450 0.45 2 tensor(5.9270, device='cuda:0', grad_fn=<DivBackward0>) tensor(5385247, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9227, device='cuda:0', grad_fn=<DivBackward0>) tensor(5447552, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9212, device='cuda:0', grad_fn=<DivBackward0>) tensor(5510521, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9198, device='cuda:0', grad_fn=<DivBackward0>) tensor(5568215, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9174, device='cuda:0', grad_fn=<DivBackward0>) tensor(5631446, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9144, device='cuda:0', grad_fn=<DivBackward0>) tensor(5693263, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9120, device='cuda:0', grad_fn=<DivBackward0>) tensor(5747177, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9078, device='cuda:0', grad_fn=<DivBackward0>) tensor(5810719, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9058, device='cuda:0', grad_fn=<DivBackward0>) tensor(5873410, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9030, device='cuda:0', grad_fn=<DivBackward0>) tensor(5932719, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9010, device='cuda:0', grad_fn=<DivBackward0>) tensor(5991313, device='cuda:0')\n",
      "(tensor(5.9011, device='cuda:0', grad_fn=<DivBackward0>), tensor(6051954, device='cuda:0'))\n",
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "50 0.05 2 tensor(6.1216, device='cuda:0', grad_fn=<DivBackward0>) tensor(59319, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0229, device='cuda:0', grad_fn=<DivBackward0>) tensor(113337, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0286, device='cuda:0', grad_fn=<DivBackward0>) tensor(172063, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0026, device='cuda:0', grad_fn=<DivBackward0>) tensor(228127, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0111, device='cuda:0', grad_fn=<DivBackward0>) tensor(287252, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0130, device='cuda:0', grad_fn=<DivBackward0>) tensor(350593, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0218, device='cuda:0', grad_fn=<DivBackward0>) tensor(414508, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0261, device='cuda:0', grad_fn=<DivBackward0>) tensor(475250, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0362, device='cuda:0', grad_fn=<DivBackward0>) tensor(535091, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0167, device='cuda:0', grad_fn=<DivBackward0>) tensor(600006, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0233, device='cuda:0', grad_fn=<DivBackward0>) tensor(662873, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0175, device='cuda:0', grad_fn=<DivBackward0>) tensor(724324, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0446, device='cuda:0', grad_fn=<DivBackward0>) tensor(783123, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0400, device='cuda:0', grad_fn=<DivBackward0>) tensor(844238, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0416, device='cuda:0', grad_fn=<DivBackward0>) tensor(906381, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0488, device='cuda:0', grad_fn=<DivBackward0>) tensor(965038, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0516, device='cuda:0', grad_fn=<DivBackward0>) tensor(1028125, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0564, device='cuda:0', grad_fn=<DivBackward0>) tensor(1087787, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0521, device='cuda:0', grad_fn=<DivBackward0>) tensor(1148690, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(6.0529, device='cuda:0', grad_fn=<DivBackward0>) tensor(1207473, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0525, device='cuda:0', grad_fn=<DivBackward0>) tensor(1271873, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0518, device='cuda:0', grad_fn=<DivBackward0>) tensor(1336341, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0482, device='cuda:0', grad_fn=<DivBackward0>) tensor(1398381, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0393, device='cuda:0', grad_fn=<DivBackward0>) tensor(1462413, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0385, device='cuda:0', grad_fn=<DivBackward0>) tensor(1522311, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0359, device='cuda:0', grad_fn=<DivBackward0>) tensor(1589389, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0327, device='cuda:0', grad_fn=<DivBackward0>) tensor(1652959, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0305, device='cuda:0', grad_fn=<DivBackward0>) tensor(1710077, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0251, device='cuda:0', grad_fn=<DivBackward0>) tensor(1772702, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0231, device='cuda:0', grad_fn=<DivBackward0>) tensor(1833432, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0241, device='cuda:0', grad_fn=<DivBackward0>) tensor(1887857, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0302, device='cuda:0', grad_fn=<DivBackward0>) tensor(1952624, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0285, device='cuda:0', grad_fn=<DivBackward0>) tensor(2005744, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0297, device='cuda:0', grad_fn=<DivBackward0>) tensor(2060124, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0273, device='cuda:0', grad_fn=<DivBackward0>) tensor(2114373, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0280, device='cuda:0', grad_fn=<DivBackward0>) tensor(2175908, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0296, device='cuda:0', grad_fn=<DivBackward0>) tensor(2241659, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0262, device='cuda:0', grad_fn=<DivBackward0>) tensor(2303502, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0258, device='cuda:0', grad_fn=<DivBackward0>) tensor(2363659, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(6.0263, device='cuda:0', grad_fn=<DivBackward0>) tensor(2427263, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0219, device='cuda:0', grad_fn=<DivBackward0>) tensor(2487704, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0192, device='cuda:0', grad_fn=<DivBackward0>) tensor(2550192, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0149, device='cuda:0', grad_fn=<DivBackward0>) tensor(2610463, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0104, device='cuda:0', grad_fn=<DivBackward0>) tensor(2672454, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0096, device='cuda:0', grad_fn=<DivBackward0>) tensor(2736722, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0055, device='cuda:0', grad_fn=<DivBackward0>) tensor(2795688, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0049, device='cuda:0', grad_fn=<DivBackward0>) tensor(2860508, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0033, device='cuda:0', grad_fn=<DivBackward0>) tensor(2923066, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0011, device='cuda:0', grad_fn=<DivBackward0>) tensor(2984716, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9967, device='cuda:0', grad_fn=<DivBackward0>) tensor(3044902, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9915, device='cuda:0', grad_fn=<DivBackward0>) tensor(3102977, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9910, device='cuda:0', grad_fn=<DivBackward0>) tensor(3167774, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9882, device='cuda:0', grad_fn=<DivBackward0>) tensor(3228171, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9844, device='cuda:0', grad_fn=<DivBackward0>) tensor(3285577, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9812, device='cuda:0', grad_fn=<DivBackward0>) tensor(3347062, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9778, device='cuda:0', grad_fn=<DivBackward0>) tensor(3407573, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9772, device='cuda:0', grad_fn=<DivBackward0>) tensor(3468256, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9753, device='cuda:0', grad_fn=<DivBackward0>) tensor(3528366, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9728, device='cuda:0', grad_fn=<DivBackward0>) tensor(3591055, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(5.9706, device='cuda:0', grad_fn=<DivBackward0>) tensor(3658053, device='cuda:0')\n",
      "50 0.05 2 tensor(5.9665, device='cuda:0', grad_fn=<DivBackward0>) tensor(3719270, device='cuda:0')\n",
      "100 0.1 2 tensor(5.9636, device='cuda:0', grad_fn=<DivBackward0>) tensor(3782912, device='cuda:0')\n",
      "150 0.15 2 tensor(5.9603, device='cuda:0', grad_fn=<DivBackward0>) tensor(3845039, device='cuda:0')\n",
      "200 0.2 2 tensor(5.9574, device='cuda:0', grad_fn=<DivBackward0>) tensor(3903897, device='cuda:0')\n",
      "250 0.25 2 tensor(5.9555, device='cuda:0', grad_fn=<DivBackward0>) tensor(3963607, device='cuda:0')\n",
      "300 0.3 2 tensor(5.9521, device='cuda:0', grad_fn=<DivBackward0>) tensor(4026376, device='cuda:0')\n",
      "350 0.35 2 tensor(5.9483, device='cuda:0', grad_fn=<DivBackward0>) tensor(4085406, device='cuda:0')\n",
      "400 0.4 2 tensor(5.9442, device='cuda:0', grad_fn=<DivBackward0>) tensor(4138843, device='cuda:0')\n",
      "450 0.45 2 tensor(5.9412, device='cuda:0', grad_fn=<DivBackward0>) tensor(4199689, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9386, device='cuda:0', grad_fn=<DivBackward0>) tensor(4260350, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9351, device='cuda:0', grad_fn=<DivBackward0>) tensor(4317688, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9321, device='cuda:0', grad_fn=<DivBackward0>) tensor(4376254, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9291, device='cuda:0', grad_fn=<DivBackward0>) tensor(4437914, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9252, device='cuda:0', grad_fn=<DivBackward0>) tensor(4506027, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9219, device='cuda:0', grad_fn=<DivBackward0>) tensor(4571439, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9203, device='cuda:0', grad_fn=<DivBackward0>) tensor(4629426, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9182, device='cuda:0', grad_fn=<DivBackward0>) tensor(4693173, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9147, device='cuda:0', grad_fn=<DivBackward0>) tensor(4753292, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9106, device='cuda:0', grad_fn=<DivBackward0>) tensor(4818399, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(5.9084, device='cuda:0', grad_fn=<DivBackward0>) tensor(4882313, device='cuda:0')\n",
      "50 0.05 2 tensor(5.9049, device='cuda:0', grad_fn=<DivBackward0>) tensor(4939305, device='cuda:0')\n",
      "100 0.1 2 tensor(5.9027, device='cuda:0', grad_fn=<DivBackward0>) tensor(5002471, device='cuda:0')\n",
      "150 0.15 2 tensor(5.9000, device='cuda:0', grad_fn=<DivBackward0>) tensor(5068820, device='cuda:0')\n",
      "200 0.2 2 tensor(5.8967, device='cuda:0', grad_fn=<DivBackward0>) tensor(5130315, device='cuda:0')\n",
      "250 0.25 2 tensor(5.8930, device='cuda:0', grad_fn=<DivBackward0>) tensor(5186703, device='cuda:0')\n",
      "300 0.3 2 tensor(5.8886, device='cuda:0', grad_fn=<DivBackward0>) tensor(5248030, device='cuda:0')\n",
      "350 0.35 2 tensor(5.8862, device='cuda:0', grad_fn=<DivBackward0>) tensor(5305972, device='cuda:0')\n",
      "400 0.4 2 tensor(5.8811, device='cuda:0', grad_fn=<DivBackward0>) tensor(5363935, device='cuda:0')\n",
      "450 0.45 2 tensor(5.8779, device='cuda:0', grad_fn=<DivBackward0>) tensor(5423947, device='cuda:0')\n",
      "500 0.5 2 tensor(5.8752, device='cuda:0', grad_fn=<DivBackward0>) tensor(5482233, device='cuda:0')\n",
      "550 0.55 2 tensor(5.8731, device='cuda:0', grad_fn=<DivBackward0>) tensor(5543507, device='cuda:0')\n",
      "600 0.6 2 tensor(5.8701, device='cuda:0', grad_fn=<DivBackward0>) tensor(5607362, device='cuda:0')\n",
      "650 0.65 2 tensor(5.8667, device='cuda:0', grad_fn=<DivBackward0>) tensor(5672440, device='cuda:0')\n",
      "700 0.7 2 tensor(5.8631, device='cuda:0', grad_fn=<DivBackward0>) tensor(5733006, device='cuda:0')\n",
      "750 0.75 2 tensor(5.8606, device='cuda:0', grad_fn=<DivBackward0>) tensor(5788092, device='cuda:0')\n",
      "800 0.8 2 tensor(5.8589, device='cuda:0', grad_fn=<DivBackward0>) tensor(5846298, device='cuda:0')\n",
      "850 0.85 2 tensor(5.8575, device='cuda:0', grad_fn=<DivBackward0>) tensor(5906498, device='cuda:0')\n",
      "900 0.9 2 tensor(5.8542, device='cuda:0', grad_fn=<DivBackward0>) tensor(5967025, device='cuda:0')\n",
      "950 0.95 2 tensor(5.8515, device='cuda:0', grad_fn=<DivBackward0>) tensor(6029425, device='cuda:0')\n",
      "(tensor(5.8489, device='cuda:0', grad_fn=<DivBackward0>), tensor(6089314, device='cuda:0'))\n",
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "50 0.05 2 tensor(6.3429, device='cuda:0', grad_fn=<DivBackward0>) tensor(56892, device='cuda:0')\n",
      "100 0.1 2 tensor(6.1199, device='cuda:0', grad_fn=<DivBackward0>) tensor(119304, device='cuda:0')\n",
      "150 0.15 2 tensor(6.1314, device='cuda:0', grad_fn=<DivBackward0>) tensor(179090, device='cuda:0')\n",
      "200 0.2 2 tensor(6.1115, device='cuda:0', grad_fn=<DivBackward0>) tensor(242391, device='cuda:0')\n",
      "250 0.25 2 tensor(6.1004, device='cuda:0', grad_fn=<DivBackward0>) tensor(296422, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0985, device='cuda:0', grad_fn=<DivBackward0>) tensor(360392, device='cuda:0')\n",
      "350 0.35 2 tensor(6.1082, device='cuda:0', grad_fn=<DivBackward0>) tensor(424821, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0985, device='cuda:0', grad_fn=<DivBackward0>) tensor(489865, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0947, device='cuda:0', grad_fn=<DivBackward0>) tensor(544365, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0810, device='cuda:0', grad_fn=<DivBackward0>) tensor(604304, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0908, device='cuda:0', grad_fn=<DivBackward0>) tensor(662243, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0793, device='cuda:0', grad_fn=<DivBackward0>) tensor(719482, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0816, device='cuda:0', grad_fn=<DivBackward0>) tensor(782259, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0813, device='cuda:0', grad_fn=<DivBackward0>) tensor(842564, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0745, device='cuda:0', grad_fn=<DivBackward0>) tensor(901942, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0680, device='cuda:0', grad_fn=<DivBackward0>) tensor(962635, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0749, device='cuda:0', grad_fn=<DivBackward0>) tensor(1025441, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0775, device='cuda:0', grad_fn=<DivBackward0>) tensor(1078335, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0687, device='cuda:0', grad_fn=<DivBackward0>) tensor(1138627, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(6.0726, device='cuda:0', grad_fn=<DivBackward0>) tensor(1198517, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0662, device='cuda:0', grad_fn=<DivBackward0>) tensor(1254772, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0600, device='cuda:0', grad_fn=<DivBackward0>) tensor(1313009, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0590, device='cuda:0', grad_fn=<DivBackward0>) tensor(1375758, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0565, device='cuda:0', grad_fn=<DivBackward0>) tensor(1436419, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0510, device='cuda:0', grad_fn=<DivBackward0>) tensor(1492947, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0492, device='cuda:0', grad_fn=<DivBackward0>) tensor(1550803, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0468, device='cuda:0', grad_fn=<DivBackward0>) tensor(1611508, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0451, device='cuda:0', grad_fn=<DivBackward0>) tensor(1675591, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0465, device='cuda:0', grad_fn=<DivBackward0>) tensor(1731088, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0475, device='cuda:0', grad_fn=<DivBackward0>) tensor(1789290, device='cuda:0')\n",
      "550 0.55 2 tensor(6.0483, device='cuda:0', grad_fn=<DivBackward0>) tensor(1850751, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0484, device='cuda:0', grad_fn=<DivBackward0>) tensor(1911367, device='cuda:0')\n",
      "650 0.65 2 tensor(6.0441, device='cuda:0', grad_fn=<DivBackward0>) tensor(1972792, device='cuda:0')\n",
      "700 0.7 2 tensor(6.0419, device='cuda:0', grad_fn=<DivBackward0>) tensor(2033253, device='cuda:0')\n",
      "750 0.75 2 tensor(6.0385, device='cuda:0', grad_fn=<DivBackward0>) tensor(2088580, device='cuda:0')\n",
      "800 0.8 2 tensor(6.0343, device='cuda:0', grad_fn=<DivBackward0>) tensor(2149432, device='cuda:0')\n",
      "850 0.85 2 tensor(6.0342, device='cuda:0', grad_fn=<DivBackward0>) tensor(2211429, device='cuda:0')\n",
      "900 0.9 2 tensor(6.0370, device='cuda:0', grad_fn=<DivBackward0>) tensor(2279810, device='cuda:0')\n",
      "950 0.95 2 tensor(6.0387, device='cuda:0', grad_fn=<DivBackward0>) tensor(2344773, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(6.0408, device='cuda:0', grad_fn=<DivBackward0>) tensor(2402557, device='cuda:0')\n",
      "50 0.05 2 tensor(6.0339, device='cuda:0', grad_fn=<DivBackward0>) tensor(2459846, device='cuda:0')\n",
      "100 0.1 2 tensor(6.0293, device='cuda:0', grad_fn=<DivBackward0>) tensor(2522766, device='cuda:0')\n",
      "150 0.15 2 tensor(6.0248, device='cuda:0', grad_fn=<DivBackward0>) tensor(2580018, device='cuda:0')\n",
      "200 0.2 2 tensor(6.0221, device='cuda:0', grad_fn=<DivBackward0>) tensor(2644474, device='cuda:0')\n",
      "250 0.25 2 tensor(6.0175, device='cuda:0', grad_fn=<DivBackward0>) tensor(2701378, device='cuda:0')\n",
      "300 0.3 2 tensor(6.0143, device='cuda:0', grad_fn=<DivBackward0>) tensor(2764881, device='cuda:0')\n",
      "350 0.35 2 tensor(6.0128, device='cuda:0', grad_fn=<DivBackward0>) tensor(2826733, device='cuda:0')\n",
      "400 0.4 2 tensor(6.0086, device='cuda:0', grad_fn=<DivBackward0>) tensor(2885499, device='cuda:0')\n",
      "450 0.45 2 tensor(6.0040, device='cuda:0', grad_fn=<DivBackward0>) tensor(2950933, device='cuda:0')\n",
      "500 0.5 2 tensor(6.0017, device='cuda:0', grad_fn=<DivBackward0>) tensor(3013464, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9998, device='cuda:0', grad_fn=<DivBackward0>) tensor(3072078, device='cuda:0')\n",
      "600 0.6 2 tensor(6.0003, device='cuda:0', grad_fn=<DivBackward0>) tensor(3139532, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9986, device='cuda:0', grad_fn=<DivBackward0>) tensor(3194793, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9968, device='cuda:0', grad_fn=<DivBackward0>) tensor(3253787, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9927, device='cuda:0', grad_fn=<DivBackward0>) tensor(3309566, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9880, device='cuda:0', grad_fn=<DivBackward0>) tensor(3367716, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9884, device='cuda:0', grad_fn=<DivBackward0>) tensor(3428918, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9868, device='cuda:0', grad_fn=<DivBackward0>) tensor(3484363, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9837, device='cuda:0', grad_fn=<DivBackward0>) tensor(3547042, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(5.9839, device='cuda:0', grad_fn=<DivBackward0>) tensor(3608634, device='cuda:0')\n",
      "50 0.05 2 tensor(5.9815, device='cuda:0', grad_fn=<DivBackward0>) tensor(3670199, device='cuda:0')\n",
      "100 0.1 2 tensor(5.9777, device='cuda:0', grad_fn=<DivBackward0>) tensor(3730717, device='cuda:0')\n",
      "150 0.15 2 tensor(5.9725, device='cuda:0', grad_fn=<DivBackward0>) tensor(3792272, device='cuda:0')\n",
      "200 0.2 2 tensor(5.9666, device='cuda:0', grad_fn=<DivBackward0>) tensor(3857284, device='cuda:0')\n",
      "250 0.25 2 tensor(5.9616, device='cuda:0', grad_fn=<DivBackward0>) tensor(3920771, device='cuda:0')\n",
      "300 0.3 2 tensor(5.9571, device='cuda:0', grad_fn=<DivBackward0>) tensor(3979864, device='cuda:0')\n",
      "350 0.35 2 tensor(5.9542, device='cuda:0', grad_fn=<DivBackward0>) tensor(4037122, device='cuda:0')\n",
      "400 0.4 2 tensor(5.9512, device='cuda:0', grad_fn=<DivBackward0>) tensor(4104202, device='cuda:0')\n",
      "450 0.45 2 tensor(5.9484, device='cuda:0', grad_fn=<DivBackward0>) tensor(4170056, device='cuda:0')\n",
      "500 0.5 2 tensor(5.9438, device='cuda:0', grad_fn=<DivBackward0>) tensor(4234432, device='cuda:0')\n",
      "550 0.55 2 tensor(5.9405, device='cuda:0', grad_fn=<DivBackward0>) tensor(4291566, device='cuda:0')\n",
      "600 0.6 2 tensor(5.9366, device='cuda:0', grad_fn=<DivBackward0>) tensor(4349809, device='cuda:0')\n",
      "650 0.65 2 tensor(5.9328, device='cuda:0', grad_fn=<DivBackward0>) tensor(4409843, device='cuda:0')\n",
      "700 0.7 2 tensor(5.9305, device='cuda:0', grad_fn=<DivBackward0>) tensor(4473791, device='cuda:0')\n",
      "750 0.75 2 tensor(5.9273, device='cuda:0', grad_fn=<DivBackward0>) tensor(4531364, device='cuda:0')\n",
      "800 0.8 2 tensor(5.9251, device='cuda:0', grad_fn=<DivBackward0>) tensor(4597392, device='cuda:0')\n",
      "850 0.85 2 tensor(5.9234, device='cuda:0', grad_fn=<DivBackward0>) tensor(4665935, device='cuda:0')\n",
      "900 0.9 2 tensor(5.9212, device='cuda:0', grad_fn=<DivBackward0>) tensor(4721091, device='cuda:0')\n",
      "950 0.95 2 tensor(5.9183, device='cuda:0', grad_fn=<DivBackward0>) tensor(4778301, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(5.9151, device='cuda:0', grad_fn=<DivBackward0>) tensor(4842470, device='cuda:0')\n",
      "50 0.05 2 tensor(5.9099, device='cuda:0', grad_fn=<DivBackward0>) tensor(4908554, device='cuda:0')\n",
      "100 0.1 2 tensor(5.9059, device='cuda:0', grad_fn=<DivBackward0>) tensor(4971879, device='cuda:0')\n",
      "150 0.15 2 tensor(5.9029, device='cuda:0', grad_fn=<DivBackward0>) tensor(5030957, device='cuda:0')\n",
      "200 0.2 2 tensor(5.8994, device='cuda:0', grad_fn=<DivBackward0>) tensor(5090331, device='cuda:0')\n",
      "250 0.25 2 tensor(5.8967, device='cuda:0', grad_fn=<DivBackward0>) tensor(5147516, device='cuda:0')\n",
      "300 0.3 2 tensor(5.8931, device='cuda:0', grad_fn=<DivBackward0>) tensor(5212988, device='cuda:0')\n",
      "350 0.35 2 tensor(5.8898, device='cuda:0', grad_fn=<DivBackward0>) tensor(5276786, device='cuda:0')\n",
      "400 0.4 2 tensor(5.8876, device='cuda:0', grad_fn=<DivBackward0>) tensor(5339148, device='cuda:0')\n",
      "450 0.45 2 tensor(5.8839, device='cuda:0', grad_fn=<DivBackward0>) tensor(5400173, device='cuda:0')\n",
      "500 0.5 2 tensor(5.8812, device='cuda:0', grad_fn=<DivBackward0>) tensor(5454342, device='cuda:0')\n",
      "550 0.55 2 tensor(5.8776, device='cuda:0', grad_fn=<DivBackward0>) tensor(5516453, device='cuda:0')\n",
      "600 0.6 2 tensor(5.8750, device='cuda:0', grad_fn=<DivBackward0>) tensor(5577829, device='cuda:0')\n",
      "650 0.65 2 tensor(5.8724, device='cuda:0', grad_fn=<DivBackward0>) tensor(5641121, device='cuda:0')\n",
      "700 0.7 2 tensor(5.8703, device='cuda:0', grad_fn=<DivBackward0>) tensor(5700352, device='cuda:0')\n",
      "750 0.75 2 tensor(5.8693, device='cuda:0', grad_fn=<DivBackward0>) tensor(5762862, device='cuda:0')\n",
      "800 0.8 2 tensor(5.8672, device='cuda:0', grad_fn=<DivBackward0>) tensor(5823264, device='cuda:0')\n",
      "850 0.85 2 tensor(5.8654, device='cuda:0', grad_fn=<DivBackward0>) tensor(5883892, device='cuda:0')\n",
      "900 0.9 2 tensor(5.8632, device='cuda:0', grad_fn=<DivBackward0>) tensor(5943578, device='cuda:0')\n",
      "950 0.95 2 tensor(5.8607, device='cuda:0', grad_fn=<DivBackward0>) tensor(6006502, device='cuda:0')\n",
      "(tensor(5.8583, device='cuda:0', grad_fn=<DivBackward0>), tensor(6074124, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "import random\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batch_size = 2\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            for i, e in enumerate(batch):\n",
    "                offset = random.randint(0, max(len(e) - 2048, 0))\n",
    "                batch[i] = e[offset:]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "epochs = 5\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import random\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "for i in range(4):\n",
    "  start_idx = 2000*i\n",
    "  end_idx = 2000*(i+1)\n",
    "  optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "  print(train_model(dataset[start_idx:end_idx], optimizer, epochs, simpleGPT2, tokenizer))\n",
    "# Loss reaches 6.3 after 3 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e57a0e7-f47b-4f17-a8be-dde9a12f4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-cd70a51e302d>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she? How it would happen. The idea from what he believes they'll end of a very bad and their members of those point for a lot of the best story are always could bring three rounds, the two days be that is just that we just a person� and only hope. The last one of a man do you could never give them for this work with different-year-term for having an extra class. After all those of people at the story has been some other than this out. Finally,\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she? That's hard to believe. She's a sweet and strong woman. She doesn't have a face like me. She's a hard look in my eyes. Like looking at my neck and smiling softly. She's never as funny as I think she is. Heterotina is not a simple phrase, I think, but is a very popular metaphor in New York magazine, especially since there are hundreds, if not thousands, of very famous and very famous people in New York. Heterot\n",
      "0 10 0 0\n",
      "1 10 tensor(5.7364, device='cuda:0') tensor(6150, device='cuda:0')\n",
      "2 10 tensor(5.7677, device='cuda:0') tensor(12072, device='cuda:0')\n",
      "3 10 tensor(5.8056, device='cuda:0') tensor(17701, device='cuda:0')\n",
      "4 10 tensor(5.8332, device='cuda:0') tensor(22832, device='cuda:0')\n",
      "5 10 tensor(5.8631, device='cuda:0') tensor(27643, device='cuda:0')\n",
      "6 10 tensor(5.8860, device='cuda:0') tensor(32385, device='cuda:0')\n",
      "7 10 tensor(5.8903, device='cuda:0') tensor(37210, device='cuda:0')\n",
      "8 10 tensor(5.8958, device='cuda:0') tensor(42070, device='cuda:0')\n",
      "9 10 tensor(5.9062, device='cuda:0') tensor(47111, device='cuda:0')\n",
      "10 10 tensor(5.9107, device='cuda:0') tensor(52337, device='cuda:0')\n",
      "11 10 tensor(5.9113, device='cuda:0') tensor(58158, device='cuda:0')\n",
      "12 10 tensor(5.9147, device='cuda:0') tensor(64330, device='cuda:0')\n",
      "13 10 tensor(5.9230, device='cuda:0') tensor(71107, device='cuda:0')\n",
      "14 10 tensor(5.9323, device='cuda:0') tensor(77771, device='cuda:0')\n",
      "15 10 tensor(5.9369, device='cuda:0') tensor(84310, device='cuda:0')\n",
      "16 10 tensor(5.9465, device='cuda:0') tensor(90440, device='cuda:0')\n",
      "17 10 tensor(5.9534, device='cuda:0') tensor(96570, device='cuda:0')\n",
      "18 10 tensor(5.9590, device='cuda:0') tensor(103026, device='cuda:0')\n",
      "19 10 tensor(5.9612, device='cuda:0') tensor(109885, device='cuda:0')\n",
      "20 10 tensor(5.9599, device='cuda:0') tensor(117341, device='cuda:0')\n",
      "21 10 tensor(5.9611, device='cuda:0') tensor(124365, device='cuda:0')\n",
      "22 10 tensor(5.9593, device='cuda:0') tensor(131298, device='cuda:0')\n",
      "23 10 tensor(5.9522, device='cuda:0') tensor(137804, device='cuda:0')\n",
      "24 10 tensor(5.9459, device='cuda:0') tensor(144491, device='cuda:0')\n",
      "25 10 tensor(5.9392, device='cuda:0') tensor(151286, device='cuda:0')\n",
      "26 10 tensor(5.9321, device='cuda:0') tensor(158129, device='cuda:0')\n",
      "27 10 tensor(5.9292, device='cuda:0') tensor(164972, device='cuda:0')\n",
      "28 10 tensor(5.9278, device='cuda:0') tensor(171515, device='cuda:0')\n",
      "29 10 tensor(5.9278, device='cuda:0') tensor(178147, device='cuda:0')\n",
      "30 10 tensor(5.9299, device='cuda:0') tensor(184434, device='cuda:0')\n",
      "31 10 tensor(5.9323, device='cuda:0') tensor(190970, device='cuda:0')\n",
      "32 10 tensor(5.9341, device='cuda:0') tensor(197585, device='cuda:0')\n",
      "33 10 tensor(5.9363, device='cuda:0') tensor(204475, device='cuda:0')\n",
      "34 10 tensor(5.9382, device='cuda:0') tensor(211492, device='cuda:0')\n",
      "35 10 tensor(5.9413, device='cuda:0') tensor(218281, device='cuda:0')\n",
      "36 10 tensor(5.9472, device='cuda:0') tensor(225245, device='cuda:0')\n",
      "37 10 tensor(5.9512, device='cuda:0') tensor(231614, device='cuda:0')\n",
      "38 10 tensor(5.9532, device='cuda:0') tensor(238102, device='cuda:0')\n",
      "39 10 tensor(5.9551, device='cuda:0') tensor(243865, device='cuda:0')\n",
      "40 10 tensor(5.9566, device='cuda:0') tensor(249973, device='cuda:0')\n",
      "41 10 tensor(5.9582, device='cuda:0') tensor(255473, device='cuda:0')\n",
      "42 10 tensor(5.9632, device='cuda:0') tensor(261392, device='cuda:0')\n",
      "43 10 tensor(5.9688, device='cuda:0') tensor(267042, device='cuda:0')\n",
      "44 10 tensor(5.9742, device='cuda:0') tensor(273097, device='cuda:0')\n",
      "45 10 tensor(5.9790, device='cuda:0') tensor(279262, device='cuda:0')\n",
      "46 10 tensor(5.9814, device='cuda:0') tensor(285196, device='cuda:0')\n",
      "47 10 tensor(5.9844, device='cuda:0') tensor(291212, device='cuda:0')\n",
      "48 10 tensor(5.9869, device='cuda:0') tensor(297861, device='cuda:0')\n",
      "49 10 tensor(5.9895, device='cuda:0') tensor(304502, device='cuda:0')\n",
      "50 10 tensor(5.9923, device='cuda:0') tensor(310906, device='cuda:0')\n",
      "51 10 tensor(5.9964, device='cuda:0') tensor(317998, device='cuda:0')\n",
      "52 10 tensor(5.9999, device='cuda:0') tensor(325090, device='cuda:0')\n",
      "53 10 tensor(6.0030, device='cuda:0') tensor(331817, device='cuda:0')\n",
      "54 10 tensor(6.0044, device='cuda:0') tensor(338544, device='cuda:0')\n",
      "55 10 tensor(6.0051, device='cuda:0') tensor(345916, device='cuda:0')\n",
      "56 10 tensor(6.0071, device='cuda:0') tensor(353223, device='cuda:0')\n",
      "57 10 tensor(6.0078, device='cuda:0') tensor(360641, device='cuda:0')\n",
      "58 10 tensor(6.0103, device='cuda:0') tensor(367353, device='cuda:0')\n",
      "59 10 tensor(6.0115, device='cuda:0') tensor(374671, device='cuda:0')\n",
      "60 10 tensor(6.0127, device='cuda:0') tensor(381466, device='cuda:0')\n",
      "61 10 tensor(6.0121, device='cuda:0') tensor(388364, device='cuda:0')\n",
      "62 10 tensor(6.0096, device='cuda:0') tensor(394457, device='cuda:0')\n",
      "63 10 tensor(6.0082, device='cuda:0') tensor(401190, device='cuda:0')\n",
      "64 10 tensor(6.0076, device='cuda:0') tensor(407923, device='cuda:0')\n",
      "65 10 tensor(6.0066, device='cuda:0') tensor(414485, device='cuda:0')\n",
      "66 10 tensor(6.0038, device='cuda:0') tensor(421598, device='cuda:0')\n",
      "67 10 tensor(6.0013, device='cuda:0') tensor(428664, device='cuda:0')\n",
      "68 10 tensor(5.9985, device='cuda:0') tensor(435707, device='cuda:0')\n",
      "69 10 tensor(5.9966, device='cuda:0') tensor(442328, device='cuda:0')\n",
      "70 10 tensor(5.9936, device='cuda:0') tensor(449266, device='cuda:0')\n",
      "71 10 tensor(5.9922, device='cuda:0') tensor(455799, device='cuda:0')\n",
      "72 10 tensor(5.9910, device='cuda:0') tensor(463137, device='cuda:0')\n",
      "73 10 tensor(5.9894, device='cuda:0') tensor(470621, device='cuda:0')\n",
      "74 10 tensor(5.9884, device='cuda:0') tensor(477479, device='cuda:0')\n",
      "75 10 tensor(5.9882, device='cuda:0') tensor(483965, device='cuda:0')\n",
      "76 10 tensor(5.9894, device='cuda:0') tensor(489770, device='cuda:0')\n",
      "77 10 tensor(5.9899, device='cuda:0') tensor(496024, device='cuda:0')\n",
      "78 10 tensor(5.9904, device='cuda:0') tensor(502638, device='cuda:0')\n",
      "79 10 tensor(5.9912, device='cuda:0') tensor(508989, device='cuda:0')\n",
      "80 10 tensor(5.9943, device='cuda:0') tensor(515682, device='cuda:0')\n",
      "81 10 tensor(5.9966, device='cuda:0') tensor(522024, device='cuda:0')\n",
      "82 10 tensor(5.9982, device='cuda:0') tensor(528366, device='cuda:0')\n",
      "83 10 tensor(6.0000, device='cuda:0') tensor(534245, device='cuda:0')\n",
      "84 10 tensor(6.0008, device='cuda:0') tensor(540438, device='cuda:0')\n",
      "85 10 tensor(6.0019, device='cuda:0') tensor(547174, device='cuda:0')\n",
      "86 10 tensor(6.0029, device='cuda:0') tensor(554515, device='cuda:0')\n",
      "87 10 tensor(6.0061, device='cuda:0') tensor(561856, device='cuda:0')\n",
      "88 10 tensor(6.0096, device='cuda:0') tensor(569566, device='cuda:0')\n",
      "89 10 tensor(6.0128, device='cuda:0') tensor(577440, device='cuda:0')\n",
      "90 10 tensor(6.0150, device='cuda:0') tensor(584629, device='cuda:0')\n",
      "91 10 tensor(6.0172, device='cuda:0') tensor(591912, device='cuda:0')\n",
      "92 10 tensor(6.0201, device='cuda:0') tensor(599195, device='cuda:0')\n",
      "93 10 tensor(6.0223, device='cuda:0') tensor(606449, device='cuda:0')\n",
      "94 10 tensor(6.0259, device='cuda:0') tensor(614015, device='cuda:0')\n",
      "95 10 tensor(6.0285, device='cuda:0') tensor(621581, device='cuda:0')\n",
      "96 10 tensor(6.0307, device='cuda:0') tensor(628898, device='cuda:0')\n",
      "97 10 tensor(6.0369, device='cuda:0') tensor(636140, device='cuda:0')\n",
      "98 10 tensor(6.0416, device='cuda:0') tensor(642804, device='cuda:0')\n",
      "99 10 tensor(6.0461, device='cuda:0') tensor(650116, device='cuda:0')\n",
      "(tensor(6.0500, device='cuda:0'), tensor(657621, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))\n",
    "print(compute_val_dataset_loss(dataset, simpleGPT2, tokenizer, 0.1))\n",
    "\n",
    "# Generations seem to be getting better, or maybe that's just my imagination.\n",
    "# Loss on val set is around 6.05, although training loss has broken 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e5090d7-b32c-4fad-8f31-55bbe60558b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(simpleGPT2, 'simpleGPT_10_epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a369956-595b-4feb-bffc-2e3b90f994fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
