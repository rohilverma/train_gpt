{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eed89d3-3348-46d6-a786-90ebb4017feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m176.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 regex-2023.3.23 tokenizers-0.13.3 transformers-4.27.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from datasets) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./.local/lib/python3.8/site-packages (from datasets) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (from datasets) (1.5.1)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas->datasets) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, urllib3, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, responses, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.3.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 urllib3-1.26.15 xxhash-3.2.0 yarl-1.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43aacda-f79c-4bdd-97ce-030ae0499c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe11b449fc54cd79943666c3ce72c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6188b35b97246a6ba484504ed031166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cfcb9c931c42ff8d79b3f462361a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6844436b4cc9479fb7e503c855a81bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c4df2a172a4633ad17822f4e624e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch as t\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa6fd8-d3d0-4dbc-be81-e531a5441970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b14a4eecd6347509f1bfb853a40c7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aaf32dc3d8483395fee5a5cd47e36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3926f59392c46f99336ec41308e2a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset openwebtext-10k/plain_text (download: 14.04 MiB, generated: 47.37 MiB, post-processed: Unknown size, total: 61.41 MiB) to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adda3fcd2b064ff1bf22ae2aced5209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/download/download_manager.py:527: FutureWarning: 'num_proc' was deprecated in version 2.6.2 and will be removed in 3.0.0. Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee33584661b4685b87d6054a48a6121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openwebtext-10k downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57394fff3b744f68195bac3f5f97ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stas/openwebtext-10k')\n",
    "dataset = ds['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700c4c5a-f2b9-4e1c-9d7d-5fa0965a73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bac5cbb-245c-473b-a5c0-25394f1568b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer: t.nn.Module):\n",
    "    if isinstance(layer, t.nn.Embedding) or isinstance(layer, t.nn.Linear):\n",
    "        layer.weight.data.normal_(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396d59ee-132c-4bd0-9a13-0c5776d0cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size = 768, context_length = 1024, dim_size = 3072, p_dropout = 0.1, n_heads = 12):\n",
    "        super().__init__()\n",
    "        self.ln_init = t.nn.LayerNorm(hidden_size)\n",
    "        self.attn = t.nn.MultiheadAttention(hidden_size, n_heads, p_dropout, batch_first = True)\n",
    "        mask = (t.triu(t.ones(context_length, context_length)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.attn_mask = t.nn.Parameter(mask, requires_grad = False)\n",
    "        self.ln_intermediate = t.nn.LayerNorm(hidden_size)\n",
    "        self.nn1 = t.nn.Linear(hidden_size, dim_size)\n",
    "        self.nn2 = t.nn.Linear(dim_size, hidden_size)\n",
    "        self.gelu = t.nn.GELU()\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        resid_0 = x\n",
    "        x = self.ln_init(x)\n",
    "        x, _ = self.attn(x, x, x, attn_mask = self.attn_mask, need_weights = False)\n",
    "        x = self.ln_intermediate(x + resid_0)\n",
    "        resid_1 = x\n",
    "        x = self.nn1(x)\n",
    "        x = self.nn2(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x + resid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33624b5b-64ba-45d3-97b7-2875ef37482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2(t.nn.Module):\n",
    "    def __init__(self, n_blocks = 1, vocab_size = 50257, context_length = 1024, hidden_size = 768, p_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.wte = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.wpe = t.nn.Embedding(context_length, hidden_size)\n",
    "        self.pe_matrix = t.nn.Parameter(t.arange(0, context_length).unsqueeze(0), requires_grad = False)\n",
    "        self.dropout = t.nn.Dropout(p_dropout)\n",
    "        self.gpt_blocks = t.nn.ModuleList([GPTBlock() for _ in range(n_blocks)])\n",
    "        self.layernorm = t.nn.LayerNorm(hidden_size)\n",
    "        self.final = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for layer in [self.wte, self.wpe, self.final]:\n",
    "            init_layer(layer)\n",
    "    \n",
    "    def forward(self, input_ids: t.Tensor, attention_mask = t.Tensor):\n",
    "        x = input_ids\n",
    "        n, seq_len = x.shape\n",
    "        hidden = self.wte(x) + self.wpe(self.pe_matrix.expand(n, -1))\n",
    "        hidden = self.dropout(hidden)\n",
    "        for gpt_block in self.gpt_blocks:\n",
    "            hidden = gpt_block(hidden)\n",
    "        hidden = self.layernorm(hidden)\n",
    "        return self.final(hidden)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072ba405-152a-44d1-b854-22cb180f8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(1024, device='cuda:0')\n",
      "torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "simpleGPT2 = t.load('simpleGPT_30_epochs.pkl')\n",
    "\n",
    "# Run model on a few truncated samples ... works!\n",
    "\n",
    "encoded_input = tokenizer(dataset[0:1], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input['attention_mask'].shape, encoded_input['attention_mask'].sum())\n",
    "logits = simpleGPT2(**encoded_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3518804f-6a4e-4c3e-b639-d06846eb5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120560209\n"
     ]
    }
   ],
   "source": [
    "# How many parameters?\n",
    "print(sum((p.numel() if p.requires_grad else 0 for p in simpleGPT2.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22e1960-82f0-4ad8-8172-7c09bf0f9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024]) tensor(21, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoded_input_alt = tokenizer(dataset[0][:100], return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "print(encoded_input_alt['attention_mask'].shape, encoded_input_alt['attention_mask'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b894e93f-04dc-42dc-9fa9-f54c88103b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "  return logits.argmax()\n",
    "\n",
    "def test_model(model, text = \"Replace me by any text you'd like.\", steps = 100, sampling = greedy_sampling, is_hf = False):\n",
    "    eos_token = \"<|endoftext|>\"\n",
    "    prompt = text\n",
    "    print(\"Starting prompt: \" + prompt)\n",
    "\n",
    "    for i in range(steps):\n",
    "        encoded_input = tokenizer([prompt], return_tensors=\"pt\", padding='max_length').to(device)\n",
    "        last_input_idx = encoded_input['attention_mask'][0].sum() - 1\n",
    "        if is_hf:\n",
    "            logits = model(**encoded_input).logits[0, last_input_idx]\n",
    "        else:\n",
    "            logits = model(**encoded_input)[0, last_input_idx]\n",
    "        next_token = sampling(logits)\n",
    "        next_string = tokenizer.decode(next_token)\n",
    "        if next_string == eos_token:\n",
    "            break\n",
    "        prompt = prompt + next_string\n",
    "    print(\"Current generation: \" + prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5d6e6d-7c3b-4849-9571-bbf505043c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-b49424ce377b>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n",
      "<ipython-input-12-b49424ce377b>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she? This is? To be some of his show, by some people, actually has long-of-old, and the other are still isn�Bately.\n",
      "\n",
      " from Stephaded, who knows, or is working while also, and he is so you care truly love for him where you did her right now. I really do every line.com. It even that is the fact that! I don�QLER\n",
      "\n",
      " from her are in your love for me a legit! I\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she? He is a son of a slave. If in your mind you are on all kinds of charges against her, if you feel like you are fighting against her, then she is acting out of your heart. She is her son. \"\n",
      "\n",
      "\n",
      "It is not his daughter who must fight with me. Once I was raped in the morning. Before the night, I was sitting in a river, and the water was cold. The water was warm, and the naked me was naked, but I\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(k):\n",
    "\n",
    "\n",
    "      def top_sampling(logits):\n",
    "          probs = t.nn.functional.softmax(logits)\n",
    "          values, indices = t.topk(probs, k)\n",
    "          index = values.multinomial(num_samples = 1, replacement = True)\n",
    "          return indices[index]\n",
    "      \n",
    "      return top_sampling\n",
    "\n",
    "# Our model generates English, but not really coherent generations.\n",
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fde2a27-c08a-4a3c-bfaa-deadcc89a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 0 0\n",
      "1 10 tensor(5.7005, device='cuda:0') tensor(6508, device='cuda:0')\n",
      "2 10 tensor(5.7236, device='cuda:0') tensor(12338, device='cuda:0')\n",
      "3 10 tensor(5.7399, device='cuda:0') tensor(18466, device='cuda:0')\n",
      "4 10 tensor(5.7029, device='cuda:0') tensor(24853, device='cuda:0')\n",
      "5 10 tensor(5.6903, device='cuda:0') tensor(30959, device='cuda:0')\n",
      "6 10 tensor(5.6737, device='cuda:0') tensor(37244, device='cuda:0')\n",
      "7 10 tensor(5.6425, device='cuda:0') tensor(43529, device='cuda:0')\n",
      "8 10 tensor(5.6160, device='cuda:0') tensor(50599, device='cuda:0')\n",
      "9 10 tensor(5.5970, device='cuda:0') tensor(57171, device='cuda:0')\n",
      "(tensor(5.5809, device='cuda:0'), tensor(63899, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits, encoded_input):\n",
    "    # logits: n x seq x d\n",
    "    # true_tokens: n x seq\n",
    "    # attention_mask = n x seq\n",
    "    true_tokens = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    valid_samples_mask = attention_mask[:, 1:].reshape(-1).bool()\n",
    "    n, seq, d  = logits.shape\n",
    "    return t.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, d)[valid_samples_mask, :], true_tokens[:, 1:].flatten()[valid_samples_mask]), valid_samples_mask.sum()\n",
    "\n",
    "def compute_dataset_loss(dataset, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    with t.no_grad():\n",
    "      n = len(dataset)\n",
    "      batch_size = 10\n",
    "      batches = n // batch_size\n",
    "      for i in range(batches):\n",
    "          print(i, batch_size, loss, samples)\n",
    "          batch = dataset[i:i+batch_size]\n",
    "          encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "          logits = model(**encoded_input)\n",
    "          # Find true labels and compute loss\n",
    "          ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "          loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "          samples = samples + valid_samples\n",
    "    return loss, samples\n",
    "\n",
    "# Compute loss of the pre-trained model on the truncated dataset\n",
    "print(compute_dataset_loss(dataset[:100], simpleGPT2, tokenizer))\n",
    "\n",
    "# Initial loss is ~5.8, same as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de997d79-36e8-462f-84fb-742b2cd7c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_dataset_loss(dataset, model, tokenizer, val_frac = 0.2):\n",
    "    n = len(dataset)\n",
    "    val_size = int(n * val_frac)\n",
    "    return compute_dataset_loss(dataset[-val_size:], model, tokenizer)\n",
    "  \n",
    "# Compute validation loss\n",
    "# print(compute_val_dataset_loss(dataset, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402e6ee2-d473-4426-b171-16598bea8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "0 0.0 2 0 0\n",
      "200 0.05 2 tensor(4.8800, device='cuda:0', grad_fn=<DivBackward0>) tensor(248722, device='cuda:0')\n",
      "400 0.1 2 tensor(4.8643, device='cuda:0', grad_fn=<DivBackward0>) tensor(494948, device='cuda:0')\n",
      "600 0.15 2 tensor(4.8672, device='cuda:0', grad_fn=<DivBackward0>) tensor(737846, device='cuda:0')\n",
      "800 0.2 2 tensor(4.8706, device='cuda:0', grad_fn=<DivBackward0>) tensor(977954, device='cuda:0')\n",
      "1000 0.25 2 tensor(4.8705, device='cuda:0', grad_fn=<DivBackward0>) tensor(1225149, device='cuda:0')\n",
      "1200 0.3 2 tensor(4.8631, device='cuda:0', grad_fn=<DivBackward0>) tensor(1462770, device='cuda:0')\n",
      "1400 0.35 2 tensor(4.8715, device='cuda:0', grad_fn=<DivBackward0>) tensor(1705108, device='cuda:0')\n",
      "1600 0.4 2 tensor(4.8775, device='cuda:0', grad_fn=<DivBackward0>) tensor(1951141, device='cuda:0')\n",
      "1800 0.45 2 tensor(4.8845, device='cuda:0', grad_fn=<DivBackward0>) tensor(2191380, device='cuda:0')\n",
      "2000 0.5 2 tensor(4.8926, device='cuda:0', grad_fn=<DivBackward0>) tensor(2434045, device='cuda:0')\n",
      "2200 0.55 2 tensor(4.9013, device='cuda:0', grad_fn=<DivBackward0>) tensor(2682954, device='cuda:0')\n",
      "2400 0.6 2 tensor(4.9122, device='cuda:0', grad_fn=<DivBackward0>) tensor(2935547, device='cuda:0')\n",
      "2600 0.65 2 tensor(4.9207, device='cuda:0', grad_fn=<DivBackward0>) tensor(3175496, device='cuda:0')\n",
      "2800 0.7 2 tensor(4.9330, device='cuda:0', grad_fn=<DivBackward0>) tensor(3427142, device='cuda:0')\n",
      "3000 0.75 2 tensor(4.9407, device='cuda:0', grad_fn=<DivBackward0>) tensor(3666169, device='cuda:0')\n",
      "3200 0.8 2 tensor(4.9520, device='cuda:0', grad_fn=<DivBackward0>) tensor(3909819, device='cuda:0')\n",
      "3400 0.85 2 tensor(4.9623, device='cuda:0', grad_fn=<DivBackward0>) tensor(4149902, device='cuda:0')\n",
      "3600 0.9 2 tensor(4.9713, device='cuda:0', grad_fn=<DivBackward0>) tensor(4391054, device='cuda:0')\n",
      "3800 0.95 2 tensor(4.9815, device='cuda:0', grad_fn=<DivBackward0>) tensor(4630406, device='cuda:0')\n",
      "Starting epoch:  1\n",
      "0 0.0 2 tensor(4.9896, device='cuda:0', grad_fn=<DivBackward0>) tensor(4869968, device='cuda:0')\n",
      "200 0.05 2 tensor(4.9908, device='cuda:0', grad_fn=<DivBackward0>) tensor(5122730, device='cuda:0')\n",
      "400 0.1 2 tensor(4.9939, device='cuda:0', grad_fn=<DivBackward0>) tensor(5358984, device='cuda:0')\n",
      "600 0.15 2 tensor(4.9982, device='cuda:0', grad_fn=<DivBackward0>) tensor(5603369, device='cuda:0')\n",
      "800 0.2 2 tensor(5.0031, device='cuda:0', grad_fn=<DivBackward0>) tensor(5834882, device='cuda:0')\n",
      "1000 0.25 2 tensor(5.0092, device='cuda:0', grad_fn=<DivBackward0>) tensor(6092529, device='cuda:0')\n",
      "1200 0.3 2 tensor(5.0129, device='cuda:0', grad_fn=<DivBackward0>) tensor(6332448, device='cuda:0')\n",
      "1400 0.35 2 tensor(5.0166, device='cuda:0', grad_fn=<DivBackward0>) tensor(6566442, device='cuda:0')\n",
      "1600 0.4 2 tensor(5.0212, device='cuda:0', grad_fn=<DivBackward0>) tensor(6809625, device='cuda:0')\n",
      "1800 0.45 2 tensor(5.0253, device='cuda:0', grad_fn=<DivBackward0>) tensor(7045696, device='cuda:0')\n",
      "2000 0.5 2 tensor(5.0289, device='cuda:0', grad_fn=<DivBackward0>) tensor(7299558, device='cuda:0')\n",
      "2200 0.55 2 tensor(5.0312, device='cuda:0', grad_fn=<DivBackward0>) tensor(7535508, device='cuda:0')\n",
      "2400 0.6 2 tensor(5.0356, device='cuda:0', grad_fn=<DivBackward0>) tensor(7778010, device='cuda:0')\n",
      "2600 0.65 2 tensor(5.0398, device='cuda:0', grad_fn=<DivBackward0>) tensor(8030429, device='cuda:0')\n",
      "2800 0.7 2 tensor(5.0441, device='cuda:0', grad_fn=<DivBackward0>) tensor(8270836, device='cuda:0')\n",
      "3000 0.75 2 tensor(5.0488, device='cuda:0', grad_fn=<DivBackward0>) tensor(8511536, device='cuda:0')\n",
      "3200 0.8 2 tensor(5.0549, device='cuda:0', grad_fn=<DivBackward0>) tensor(8759742, device='cuda:0')\n",
      "3400 0.85 2 tensor(5.0581, device='cuda:0', grad_fn=<DivBackward0>) tensor(8997073, device='cuda:0')\n",
      "3600 0.9 2 tensor(5.0625, device='cuda:0', grad_fn=<DivBackward0>) tensor(9238610, device='cuda:0')\n",
      "3800 0.95 2 tensor(5.0667, device='cuda:0', grad_fn=<DivBackward0>) tensor(9470012, device='cuda:0')\n",
      "Starting epoch:  2\n",
      "0 0.0 2 tensor(5.0714, device='cuda:0', grad_fn=<DivBackward0>) tensor(9711973, device='cuda:0')\n",
      "200 0.05 2 tensor(5.0700, device='cuda:0', grad_fn=<DivBackward0>) tensor(9948207, device='cuda:0')\n",
      "400 0.1 2 tensor(5.0690, device='cuda:0', grad_fn=<DivBackward0>) tensor(10174308, device='cuda:0')\n",
      "600 0.15 2 tensor(5.0691, device='cuda:0', grad_fn=<DivBackward0>) tensor(10426966, device='cuda:0')\n",
      "800 0.2 2 tensor(5.0693, device='cuda:0', grad_fn=<DivBackward0>) tensor(10672488, device='cuda:0')\n",
      "1000 0.25 2 tensor(5.0701, device='cuda:0', grad_fn=<DivBackward0>) tensor(10914899, device='cuda:0')\n",
      "1200 0.3 2 tensor(5.0701, device='cuda:0', grad_fn=<DivBackward0>) tensor(11155594, device='cuda:0')\n",
      "1400 0.35 2 tensor(5.0707, device='cuda:0', grad_fn=<DivBackward0>) tensor(11403182, device='cuda:0')\n",
      "1600 0.4 2 tensor(5.0714, device='cuda:0', grad_fn=<DivBackward0>) tensor(11654947, device='cuda:0')\n",
      "1800 0.45 2 tensor(5.0712, device='cuda:0', grad_fn=<DivBackward0>) tensor(11898880, device='cuda:0')\n",
      "2000 0.5 2 tensor(5.0708, device='cuda:0', grad_fn=<DivBackward0>) tensor(12151168, device='cuda:0')\n",
      "2200 0.55 2 tensor(5.0717, device='cuda:0', grad_fn=<DivBackward0>) tensor(12400040, device='cuda:0')\n",
      "2400 0.6 2 tensor(5.0725, device='cuda:0', grad_fn=<DivBackward0>) tensor(12643144, device='cuda:0')\n",
      "2600 0.65 2 tensor(5.0736, device='cuda:0', grad_fn=<DivBackward0>) tensor(12883107, device='cuda:0')\n",
      "2800 0.7 2 tensor(5.0740, device='cuda:0', grad_fn=<DivBackward0>) tensor(13115494, device='cuda:0')\n",
      "3000 0.75 2 tensor(5.0743, device='cuda:0', grad_fn=<DivBackward0>) tensor(13360940, device='cuda:0')\n",
      "3200 0.8 2 tensor(5.0753, device='cuda:0', grad_fn=<DivBackward0>) tensor(13611277, device='cuda:0')\n",
      "3400 0.85 2 tensor(5.0754, device='cuda:0', grad_fn=<DivBackward0>) tensor(13849593, device='cuda:0')\n",
      "3600 0.9 2 tensor(5.0760, device='cuda:0', grad_fn=<DivBackward0>) tensor(14095968, device='cuda:0')\n",
      "3800 0.95 2 tensor(5.0758, device='cuda:0', grad_fn=<DivBackward0>) tensor(14332273, device='cuda:0')\n",
      "Starting epoch:  3\n",
      "0 0.0 2 tensor(5.0761, device='cuda:0', grad_fn=<DivBackward0>) tensor(14587821, device='cuda:0')\n",
      "200 0.05 2 tensor(5.0733, device='cuda:0', grad_fn=<DivBackward0>) tensor(14829299, device='cuda:0')\n",
      "400 0.1 2 tensor(5.0707, device='cuda:0', grad_fn=<DivBackward0>) tensor(15056151, device='cuda:0')\n",
      "600 0.15 2 tensor(5.0693, device='cuda:0', grad_fn=<DivBackward0>) tensor(15308826, device='cuda:0')\n",
      "800 0.2 2 tensor(5.0676, device='cuda:0', grad_fn=<DivBackward0>) tensor(15547280, device='cuda:0')\n",
      "1000 0.25 2 tensor(5.0659, device='cuda:0', grad_fn=<DivBackward0>) tensor(15794478, device='cuda:0')\n",
      "1200 0.3 2 tensor(5.0654, device='cuda:0', grad_fn=<DivBackward0>) tensor(16035624, device='cuda:0')\n",
      "1400 0.35 2 tensor(5.0631, device='cuda:0', grad_fn=<DivBackward0>) tensor(16274200, device='cuda:0')\n",
      "1600 0.4 2 tensor(5.0615, device='cuda:0', grad_fn=<DivBackward0>) tensor(16520210, device='cuda:0')\n",
      "1800 0.45 2 tensor(5.0595, device='cuda:0', grad_fn=<DivBackward0>) tensor(16753308, device='cuda:0')\n",
      "2000 0.5 2 tensor(5.0580, device='cuda:0', grad_fn=<DivBackward0>) tensor(16992432, device='cuda:0')\n",
      "2200 0.55 2 tensor(5.0566, device='cuda:0', grad_fn=<DivBackward0>) tensor(17230976, device='cuda:0')\n",
      "2400 0.6 2 tensor(5.0554, device='cuda:0', grad_fn=<DivBackward0>) tensor(17478199, device='cuda:0')\n",
      "2600 0.65 2 tensor(5.0546, device='cuda:0', grad_fn=<DivBackward0>) tensor(17730221, device='cuda:0')\n",
      "2800 0.7 2 tensor(5.0531, device='cuda:0', grad_fn=<DivBackward0>) tensor(17971966, device='cuda:0')\n",
      "3000 0.75 2 tensor(5.0521, device='cuda:0', grad_fn=<DivBackward0>) tensor(18210356, device='cuda:0')\n",
      "3200 0.8 2 tensor(5.0502, device='cuda:0', grad_fn=<DivBackward0>) tensor(18451676, device='cuda:0')\n",
      "3400 0.85 2 tensor(5.0491, device='cuda:0', grad_fn=<DivBackward0>) tensor(18691489, device='cuda:0')\n",
      "3600 0.9 2 tensor(5.0480, device='cuda:0', grad_fn=<DivBackward0>) tensor(18929603, device='cuda:0')\n",
      "3800 0.95 2 tensor(5.0464, device='cuda:0', grad_fn=<DivBackward0>) tensor(19170915, device='cuda:0')\n",
      "Starting epoch:  4\n",
      "0 0.0 2 tensor(5.0459, device='cuda:0', grad_fn=<DivBackward0>) tensor(19422894, device='cuda:0')\n",
      "200 0.05 2 tensor(5.0428, device='cuda:0', grad_fn=<DivBackward0>) tensor(19662806, device='cuda:0')\n",
      "400 0.1 2 tensor(5.0400, device='cuda:0', grad_fn=<DivBackward0>) tensor(19898472, device='cuda:0')\n",
      "600 0.15 2 tensor(5.0370, device='cuda:0', grad_fn=<DivBackward0>) tensor(20124481, device='cuda:0')\n",
      "800 0.2 2 tensor(5.0358, device='cuda:0', grad_fn=<DivBackward0>) tensor(20371535, device='cuda:0')\n",
      "1000 0.25 2 tensor(5.0336, device='cuda:0', grad_fn=<DivBackward0>) tensor(20611473, device='cuda:0')\n",
      "1200 0.3 2 tensor(5.0319, device='cuda:0', grad_fn=<DivBackward0>) tensor(20859874, device='cuda:0')\n",
      "1400 0.35 2 tensor(5.0297, device='cuda:0', grad_fn=<DivBackward0>) tensor(21110631, device='cuda:0')\n",
      "1600 0.4 2 tensor(5.0289, device='cuda:0', grad_fn=<DivBackward0>) tensor(21362697, device='cuda:0')\n",
      "1800 0.45 2 tensor(5.0271, device='cuda:0', grad_fn=<DivBackward0>) tensor(21600709, device='cuda:0')\n",
      "2000 0.5 2 tensor(5.0246, device='cuda:0', grad_fn=<DivBackward0>) tensor(21831970, device='cuda:0')\n",
      "2200 0.55 2 tensor(5.0234, device='cuda:0', grad_fn=<DivBackward0>) tensor(22067504, device='cuda:0')\n",
      "2400 0.6 2 tensor(5.0215, device='cuda:0', grad_fn=<DivBackward0>) tensor(22318394, device='cuda:0')\n",
      "2600 0.65 2 tensor(5.0195, device='cuda:0', grad_fn=<DivBackward0>) tensor(22561794, device='cuda:0')\n",
      "2800 0.7 2 tensor(5.0177, device='cuda:0', grad_fn=<DivBackward0>) tensor(22811530, device='cuda:0')\n",
      "3000 0.75 2 tensor(5.0158, device='cuda:0', grad_fn=<DivBackward0>) tensor(23052230, device='cuda:0')\n",
      "3200 0.8 2 tensor(5.0138, device='cuda:0', grad_fn=<DivBackward0>) tensor(23293933, device='cuda:0')\n",
      "3400 0.85 2 tensor(5.0125, device='cuda:0', grad_fn=<DivBackward0>) tensor(23545144, device='cuda:0')\n",
      "3600 0.9 2 tensor(5.0098, device='cuda:0', grad_fn=<DivBackward0>) tensor(23776880, device='cuda:0')\n",
      "3800 0.95 2 tensor(5.0082, device='cuda:0', grad_fn=<DivBackward0>) tensor(24020691, device='cuda:0')\n",
      "(tensor(5.0068, device='cuda:0', grad_fn=<DivBackward0>), tensor(24262608, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on a subset of training set, and then evaluate on val set\n",
    "import random\n",
    "\n",
    "def train_model(dataset, optimizer, epochs, model, tokenizer):\n",
    "    loss = 0\n",
    "    samples = 0\n",
    "    n = len(dataset)\n",
    "    batch_size = 2\n",
    "    batches = n // batch_size\n",
    "    print_interval = batches // 20\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, max_lr = 2.5e-4, total_steps = epochs * batches, pct_start = 0.2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(dataset)\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        for i in range(batches):\n",
    "            if i % print_interval == 0:\n",
    "                print(i, i/float(batches), batch_size, loss, samples)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            for i, e in enumerate(batch):\n",
    "                offset = random.randint(0, max(len(e) - 2048, 0))\n",
    "                batch[i] = e[offset:]\n",
    "            encoded_input = tokenizer(batch, return_tensors='pt', padding='max_length', truncation=True).to(device)\n",
    "            logits = model(**encoded_input)\n",
    "\n",
    "            # Find true labels and compute loss\n",
    "            ce_loss, valid_samples = loss_fn(logits, encoded_input)\n",
    "            loss = (loss * samples + ce_loss * valid_samples ) / (samples + valid_samples)\n",
    "            samples = samples + valid_samples\n",
    "\n",
    "            # Backprop\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    return loss, samples\n",
    "\n",
    "epochs = 5\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import random\n",
    "\n",
    "lrs = [5e-5, 5e-4, 1e-5, 2e-5]\n",
    "\n",
    "optimizer = Adam(simpleGPT2.parameters(), lr = lrs[-1])\n",
    "print(train_model(dataset[:2000*4], optimizer, epochs, simpleGPT2, tokenizer))\n",
    "# Loss reaches 6.3 after 3 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e57a0e7-f47b-4f17-a8be-dde9a12f4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: Mary is the greatest. Or is she?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-b49424ce377b>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = t.nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: Mary is the greatest. Or is she?�s, and respect to be �ive from a few words, and you take a secret for using the idea. Sometimes he had trouble, and is a kind of. And it�G. When he kept the whole is great, he doesn't know.J. She really excited, it has done and was the ability. He�s so it.I�. He then about for doing anything and I was not have I'm glad things right now I�s and learn an out\n",
      "Starting prompt: Mary is the greatest. Or is she?\n",
      "Current generation: Mary is the greatest. Or is she? And if so, what is your desire?\n",
      "\n",
      "\n",
      "W. S. RYAN. There would be nothing that would turn out to be agreeable if there were no more than four-foot-one men-in-arms.\n",
      "\n",
      "\n",
      "S. RYAN. No; three; twelve; twenty-one but a hundred to fourteen.\n",
      "\n",
      "\n",
      "NORTH CAROLINA. Is there a desire like yours that a man's life will not be saved if he has to suffer?\n",
      "0 10 0 0\n",
      "1 10 tensor(5.4454, device='cuda:0') tensor(6150, device='cuda:0')\n",
      "2 10 tensor(5.4869, device='cuda:0') tensor(12072, device='cuda:0')\n",
      "3 10 tensor(5.5366, device='cuda:0') tensor(17701, device='cuda:0')\n",
      "4 10 tensor(5.5714, device='cuda:0') tensor(22832, device='cuda:0')\n",
      "5 10 tensor(5.6015, device='cuda:0') tensor(27643, device='cuda:0')\n",
      "6 10 tensor(5.6236, device='cuda:0') tensor(32385, device='cuda:0')\n",
      "7 10 tensor(5.6327, device='cuda:0') tensor(37210, device='cuda:0')\n",
      "8 10 tensor(5.6449, device='cuda:0') tensor(42070, device='cuda:0')\n",
      "9 10 tensor(5.6601, device='cuda:0') tensor(47111, device='cuda:0')\n",
      "10 10 tensor(5.6684, device='cuda:0') tensor(52337, device='cuda:0')\n",
      "11 10 tensor(5.6734, device='cuda:0') tensor(58158, device='cuda:0')\n",
      "12 10 tensor(5.6793, device='cuda:0') tensor(64330, device='cuda:0')\n",
      "13 10 tensor(5.6879, device='cuda:0') tensor(71107, device='cuda:0')\n",
      "14 10 tensor(5.6974, device='cuda:0') tensor(77771, device='cuda:0')\n",
      "15 10 tensor(5.7057, device='cuda:0') tensor(84310, device='cuda:0')\n",
      "16 10 tensor(5.7193, device='cuda:0') tensor(90440, device='cuda:0')\n",
      "17 10 tensor(5.7269, device='cuda:0') tensor(96570, device='cuda:0')\n",
      "18 10 tensor(5.7332, device='cuda:0') tensor(103026, device='cuda:0')\n",
      "19 10 tensor(5.7374, device='cuda:0') tensor(109885, device='cuda:0')\n",
      "20 10 tensor(5.7369, device='cuda:0') tensor(117341, device='cuda:0')\n",
      "21 10 tensor(5.7399, device='cuda:0') tensor(124365, device='cuda:0')\n",
      "22 10 tensor(5.7376, device='cuda:0') tensor(131298, device='cuda:0')\n",
      "23 10 tensor(5.7304, device='cuda:0') tensor(137804, device='cuda:0')\n",
      "24 10 tensor(5.7235, device='cuda:0') tensor(144491, device='cuda:0')\n",
      "25 10 tensor(5.7162, device='cuda:0') tensor(151286, device='cuda:0')\n",
      "26 10 tensor(5.7080, device='cuda:0') tensor(158129, device='cuda:0')\n",
      "27 10 tensor(5.7056, device='cuda:0') tensor(164972, device='cuda:0')\n",
      "28 10 tensor(5.7048, device='cuda:0') tensor(171515, device='cuda:0')\n",
      "29 10 tensor(5.7062, device='cuda:0') tensor(178147, device='cuda:0')\n",
      "30 10 tensor(5.7095, device='cuda:0') tensor(184434, device='cuda:0')\n",
      "31 10 tensor(5.7129, device='cuda:0') tensor(190970, device='cuda:0')\n",
      "32 10 tensor(5.7159, device='cuda:0') tensor(197585, device='cuda:0')\n",
      "33 10 tensor(5.7195, device='cuda:0') tensor(204475, device='cuda:0')\n",
      "34 10 tensor(5.7235, device='cuda:0') tensor(211492, device='cuda:0')\n",
      "35 10 tensor(5.7275, device='cuda:0') tensor(218281, device='cuda:0')\n",
      "36 10 tensor(5.7347, device='cuda:0') tensor(225245, device='cuda:0')\n",
      "37 10 tensor(5.7389, device='cuda:0') tensor(231614, device='cuda:0')\n",
      "38 10 tensor(5.7410, device='cuda:0') tensor(238102, device='cuda:0')\n",
      "39 10 tensor(5.7426, device='cuda:0') tensor(243865, device='cuda:0')\n",
      "40 10 tensor(5.7434, device='cuda:0') tensor(249973, device='cuda:0')\n",
      "41 10 tensor(5.7444, device='cuda:0') tensor(255473, device='cuda:0')\n",
      "42 10 tensor(5.7489, device='cuda:0') tensor(261392, device='cuda:0')\n",
      "43 10 tensor(5.7530, device='cuda:0') tensor(267042, device='cuda:0')\n",
      "44 10 tensor(5.7580, device='cuda:0') tensor(273097, device='cuda:0')\n",
      "45 10 tensor(5.7623, device='cuda:0') tensor(279262, device='cuda:0')\n",
      "46 10 tensor(5.7642, device='cuda:0') tensor(285196, device='cuda:0')\n",
      "47 10 tensor(5.7668, device='cuda:0') tensor(291212, device='cuda:0')\n",
      "48 10 tensor(5.7693, device='cuda:0') tensor(297861, device='cuda:0')\n",
      "49 10 tensor(5.7716, device='cuda:0') tensor(304502, device='cuda:0')\n",
      "50 10 tensor(5.7747, device='cuda:0') tensor(310906, device='cuda:0')\n",
      "51 10 tensor(5.7788, device='cuda:0') tensor(317998, device='cuda:0')\n",
      "52 10 tensor(5.7836, device='cuda:0') tensor(325090, device='cuda:0')\n",
      "53 10 tensor(5.7878, device='cuda:0') tensor(331817, device='cuda:0')\n",
      "54 10 tensor(5.7903, device='cuda:0') tensor(338544, device='cuda:0')\n",
      "55 10 tensor(5.7925, device='cuda:0') tensor(345916, device='cuda:0')\n",
      "56 10 tensor(5.7958, device='cuda:0') tensor(353223, device='cuda:0')\n",
      "57 10 tensor(5.7976, device='cuda:0') tensor(360641, device='cuda:0')\n",
      "58 10 tensor(5.8005, device='cuda:0') tensor(367353, device='cuda:0')\n",
      "59 10 tensor(5.8025, device='cuda:0') tensor(374671, device='cuda:0')\n",
      "60 10 tensor(5.8042, device='cuda:0') tensor(381466, device='cuda:0')\n",
      "61 10 tensor(5.8044, device='cuda:0') tensor(388364, device='cuda:0')\n",
      "62 10 tensor(5.8027, device='cuda:0') tensor(394457, device='cuda:0')\n",
      "63 10 tensor(5.8019, device='cuda:0') tensor(401190, device='cuda:0')\n",
      "64 10 tensor(5.8015, device='cuda:0') tensor(407923, device='cuda:0')\n",
      "65 10 tensor(5.8006, device='cuda:0') tensor(414485, device='cuda:0')\n",
      "66 10 tensor(5.7981, device='cuda:0') tensor(421598, device='cuda:0')\n",
      "67 10 tensor(5.7954, device='cuda:0') tensor(428664, device='cuda:0')\n",
      "68 10 tensor(5.7924, device='cuda:0') tensor(435707, device='cuda:0')\n",
      "69 10 tensor(5.7902, device='cuda:0') tensor(442328, device='cuda:0')\n",
      "70 10 tensor(5.7871, device='cuda:0') tensor(449266, device='cuda:0')\n",
      "71 10 tensor(5.7851, device='cuda:0') tensor(455799, device='cuda:0')\n",
      "72 10 tensor(5.7835, device='cuda:0') tensor(463137, device='cuda:0')\n",
      "73 10 tensor(5.7817, device='cuda:0') tensor(470621, device='cuda:0')\n",
      "74 10 tensor(5.7805, device='cuda:0') tensor(477479, device='cuda:0')\n",
      "75 10 tensor(5.7797, device='cuda:0') tensor(483965, device='cuda:0')\n",
      "76 10 tensor(5.7804, device='cuda:0') tensor(489770, device='cuda:0')\n",
      "77 10 tensor(5.7806, device='cuda:0') tensor(496024, device='cuda:0')\n",
      "78 10 tensor(5.7811, device='cuda:0') tensor(502638, device='cuda:0')\n",
      "79 10 tensor(5.7819, device='cuda:0') tensor(508989, device='cuda:0')\n",
      "80 10 tensor(5.7850, device='cuda:0') tensor(515682, device='cuda:0')\n",
      "81 10 tensor(5.7874, device='cuda:0') tensor(522024, device='cuda:0')\n",
      "82 10 tensor(5.7891, device='cuda:0') tensor(528366, device='cuda:0')\n",
      "83 10 tensor(5.7910, device='cuda:0') tensor(534245, device='cuda:0')\n",
      "84 10 tensor(5.7920, device='cuda:0') tensor(540438, device='cuda:0')\n",
      "85 10 tensor(5.7936, device='cuda:0') tensor(547174, device='cuda:0')\n",
      "86 10 tensor(5.7952, device='cuda:0') tensor(554515, device='cuda:0')\n",
      "87 10 tensor(5.7989, device='cuda:0') tensor(561856, device='cuda:0')\n",
      "88 10 tensor(5.8029, device='cuda:0') tensor(569566, device='cuda:0')\n",
      "89 10 tensor(5.8063, device='cuda:0') tensor(577440, device='cuda:0')\n",
      "90 10 tensor(5.8088, device='cuda:0') tensor(584629, device='cuda:0')\n",
      "91 10 tensor(5.8115, device='cuda:0') tensor(591912, device='cuda:0')\n",
      "92 10 tensor(5.8147, device='cuda:0') tensor(599195, device='cuda:0')\n",
      "93 10 tensor(5.8172, device='cuda:0') tensor(606449, device='cuda:0')\n",
      "94 10 tensor(5.8207, device='cuda:0') tensor(614015, device='cuda:0')\n",
      "95 10 tensor(5.8230, device='cuda:0') tensor(621581, device='cuda:0')\n",
      "96 10 tensor(5.8254, device='cuda:0') tensor(628898, device='cuda:0')\n",
      "97 10 tensor(5.8316, device='cuda:0') tensor(636140, device='cuda:0')\n",
      "98 10 tensor(5.8361, device='cuda:0') tensor(642804, device='cuda:0')\n",
      "99 10 tensor(5.8403, device='cuda:0') tensor(650116, device='cuda:0')\n",
      "(tensor(5.8439, device='cuda:0'), tensor(657621, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "test_model(simpleGPT2, text = \"Mary is the greatest. Or is she?\", steps = 100, sampling = top_k_sampling(100))\n",
    "test_model(pretrained_model, text = \"Mary is the greatest. Or is she?\", is_hf = True, steps = 100, sampling = top_k_sampling(100))\n",
    "print(compute_val_dataset_loss(dataset, simpleGPT2, tokenizer, 0.1))\n",
    "\n",
    "# Generations seem to be getting better, or maybe that's just my imagination.\n",
    "# Loss on val set is around 5.84, although training loss ~5.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e5090d7-b32c-4fad-8f31-55bbe60558b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(simpleGPT2, 'simpleGPT_55_epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a369956-595b-4feb-bffc-2e3b90f994fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
